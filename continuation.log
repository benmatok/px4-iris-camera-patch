WarpDrive not installed or not fully importable (likely missing pycuda). Using custom CPU trainer.
WARNING: CUDA or WarpDrive not available. Falling back to Custom CPU Training.
CPU Mode: Reduced agents to 200 and iterations to 1000
Loading checkpoint from final_policy.pth
Checkpoint loaded successfully.
Starting Iteration 0
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 0: Reward 1.196 Loss 11.439 AE 0.023
Saved checkpoint to policy_0.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 1
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 1: Reward 1.149 Loss 13.683 AE 0.033
Starting Iteration 2
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 2: Reward 1.129 Loss 13.532 AE 0.031
Starting Iteration 3
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 3: Reward 1.081 Loss 13.211 AE 0.029
Starting Iteration 4
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 4: Reward 1.046 Loss 14.002 AE 0.028
Starting Iteration 5
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 5: Reward 1.053 Loss 13.778 AE 0.026
Starting Iteration 6
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 6: Reward 1.069 Loss 13.027 AE 0.026
Starting Iteration 7
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 7: Reward 1.135 Loss 16.108 AE 0.025
Starting Iteration 8
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 8: Reward 1.143 Loss 14.096 AE 0.026
Starting Iteration 9
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 9: Reward 1.196 Loss 12.135 AE 0.029
Starting Iteration 10
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 10: Reward 1.183 Loss 10.494 AE 0.031
Starting Iteration 11
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 11: Reward 1.147 Loss 10.002 AE 0.033
Starting Iteration 12
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 12: Reward 1.125 Loss 13.374 AE 0.036
Starting Iteration 13
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 13: Reward 1.111 Loss 13.970 AE 0.037
Starting Iteration 14
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 14: Reward 1.122 Loss 14.062 AE 0.037
Starting Iteration 15
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 15: Reward 1.086 Loss 11.929 AE 0.036
Starting Iteration 16
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 16: Reward 1.116 Loss 10.156 AE 0.035
Starting Iteration 17
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 17: Reward 1.094 Loss 8.845 AE 0.032
Starting Iteration 18
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 18: Reward 1.132 Loss 10.616 AE 0.030
Starting Iteration 19
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 19: Reward 1.157 Loss 11.689 AE 0.028
Starting Iteration 20
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 20: Reward 1.178 Loss 12.706 AE 0.027
Starting Iteration 21
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 21: Reward 1.166 Loss 11.698 AE 0.026
Starting Iteration 22
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 22: Reward 1.166 Loss 12.855 AE 0.026
Starting Iteration 23
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 23: Reward 1.176 Loss 11.244 AE 0.027
Starting Iteration 24
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 24: Reward 1.156 Loss 10.405 AE 0.027
Starting Iteration 25
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 25: Reward 1.175 Loss 10.311 AE 0.027
Starting Iteration 26
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 26: Reward 1.164 Loss 9.914 AE 0.027
Starting Iteration 27
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 27: Reward 1.164 Loss 10.833 AE 0.028
Starting Iteration 28
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 28: Reward 1.127 Loss 11.159 AE 0.026
Starting Iteration 29
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 29: Reward 1.162 Loss 10.938 AE 0.027
Starting Iteration 30
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 30: Reward 1.149 Loss 10.328 AE 0.026
Starting Iteration 31
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 31: Reward 1.182 Loss 10.262 AE 0.026
Starting Iteration 32
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 32: Reward 1.134 Loss 9.713 AE 0.026
Starting Iteration 33
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 33: Reward 1.144 Loss 10.120 AE 0.027
Starting Iteration 34
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 34: Reward 1.119 Loss 10.117 AE 0.026
Starting Iteration 35
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 35: Reward 1.148 Loss 10.908 AE 0.027
Starting Iteration 36
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 36: Reward 1.150 Loss 10.416 AE 0.027
Starting Iteration 37
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 37: Reward 1.144 Loss 9.837 AE 0.028
Starting Iteration 38
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 38: Reward 1.171 Loss 10.989 AE 0.028
Starting Iteration 39
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 39: Reward 1.150 Loss 10.644 AE 0.026
Starting Iteration 40
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 40: Reward 1.167 Loss 11.446 AE 0.023
Starting Iteration 41
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 41: Reward 1.132 Loss 10.079 AE 0.023
Starting Iteration 42
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 42: Reward 1.153 Loss 11.151 AE 0.021
Starting Iteration 43
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 43: Reward 1.136 Loss 10.707 AE 0.021
Starting Iteration 44
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 44: Reward 1.138 Loss 10.494 AE 0.022
Starting Iteration 45
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 45: Reward 1.138 Loss 11.457 AE 0.023
Starting Iteration 46
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 46: Reward 1.114 Loss 10.329 AE 0.023
Starting Iteration 47
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 47: Reward 1.152 Loss 10.851 AE 0.024
Starting Iteration 48
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 48: Reward 1.159 Loss 10.500 AE 0.025
Starting Iteration 49
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 49: Reward 1.173 Loss 10.570 AE 0.025
Starting Iteration 50
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 50: Reward 1.153 Loss 9.197 AE 0.027
Saved checkpoint to policy_50.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 51
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 51: Reward 1.136 Loss 9.517 AE 0.027
Starting Iteration 52
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 52: Reward 1.142 Loss 10.241 AE 0.028
Starting Iteration 53
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 53: Reward 1.138 Loss 10.174 AE 0.029
Starting Iteration 54
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 54: Reward 1.131 Loss 9.149 AE 0.028
Starting Iteration 55
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 55: Reward 1.108 Loss 9.064 AE 0.028
Starting Iteration 56
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 56: Reward 1.179 Loss 9.976 AE 0.028
Starting Iteration 57
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 57: Reward 1.147 Loss 9.591 AE 0.027
Starting Iteration 58
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 58: Reward 1.149 Loss 9.560 AE 0.028
Starting Iteration 59
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 59: Reward 1.143 Loss 8.861 AE 0.030
Starting Iteration 60
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 60: Reward 1.119 Loss 9.006 AE 0.031
Starting Iteration 61
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 61: Reward 1.129 Loss 9.267 AE 0.032
Starting Iteration 62
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 62: Reward 1.130 Loss 9.732 AE 0.033
Starting Iteration 63
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 63: Reward 1.116 Loss 8.766 AE 0.031
Starting Iteration 64
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 64: Reward 1.137 Loss 9.626 AE 0.032
Starting Iteration 65
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 65: Reward 1.111 Loss 8.452 AE 0.031
Starting Iteration 66
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 66: Reward 1.154 Loss 9.941 AE 0.031
Starting Iteration 67
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 67: Reward 1.121 Loss 9.384 AE 0.030
Starting Iteration 68
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 68: Reward 1.153 Loss 9.891 AE 0.031
Starting Iteration 69
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 69: Reward 1.156 Loss 9.834 AE 0.030
Starting Iteration 70
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 70: Reward 1.127 Loss 9.213 AE 0.030
Starting Iteration 71
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 71: Reward 1.144 Loss 9.664 AE 0.031
Starting Iteration 72
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 72: Reward 1.148 Loss 9.118 AE 0.030
Starting Iteration 73
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 73: Reward 1.180 Loss 9.089 AE 0.030
Starting Iteration 74
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 74: Reward 1.151 Loss 8.528 AE 0.029
Starting Iteration 75
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 75: Reward 1.130 Loss 8.612 AE 0.028
Starting Iteration 76
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 76: Reward 1.134 Loss 9.112 AE 0.028
Starting Iteration 77
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 77: Reward 1.110 Loss 8.171 AE 0.029
Starting Iteration 78
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 78: Reward 1.135 Loss 8.676 AE 0.029
Starting Iteration 79
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 79: Reward 1.133 Loss 8.850 AE 0.028
Starting Iteration 80
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 80: Reward 1.142 Loss 9.483 AE 0.028
Starting Iteration 81
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 81: Reward 1.158 Loss 8.772 AE 0.027
Starting Iteration 82
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 82: Reward 1.155 Loss 9.720 AE 0.026
Starting Iteration 83
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 83: Reward 1.148 Loss 9.085 AE 0.025
Starting Iteration 84
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 84: Reward 1.142 Loss 9.610 AE 0.024
Starting Iteration 85
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 85: Reward 1.144 Loss 9.427 AE 0.024
Starting Iteration 86
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 86: Reward 1.184 Loss 10.936 AE 0.023
Starting Iteration 87
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 87: Reward 1.145 Loss 10.265 AE 0.023
Starting Iteration 88
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 88: Reward 1.177 Loss 11.918 AE 0.023
Starting Iteration 89
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 89: Reward 1.162 Loss 11.208 AE 0.023
Starting Iteration 90
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 90: Reward 1.187 Loss 10.851 AE 0.023
Starting Iteration 91
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 91: Reward 1.161 Loss 10.818 AE 0.023
Starting Iteration 92
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 92: Reward 1.141 Loss 11.025 AE 0.023
Starting Iteration 93
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 93: Reward 1.154 Loss 10.667 AE 0.023
Starting Iteration 94
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 94: Reward 1.167 Loss 10.742 AE 0.023
Starting Iteration 95
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 95: Reward 1.125 Loss 9.453 AE 0.023
Starting Iteration 96
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 96: Reward 1.168 Loss 10.165 AE 0.025
Starting Iteration 97
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 97: Reward 1.140 Loss 8.620 AE 0.026
Starting Iteration 98
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 98: Reward 1.161 Loss 9.964 AE 0.026
Starting Iteration 99
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 99: Reward 1.188 Loss 9.168 AE 0.028
Starting Iteration 100
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 100: Reward 1.137 Loss 8.499 AE 0.029
Saved checkpoint to policy_100.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 101
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 101: Reward 1.172 Loss 9.470 AE 0.029
Starting Iteration 102
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 102: Reward 1.147 Loss 9.388 AE 0.029
Starting Iteration 103
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 103: Reward 1.152 Loss 9.854 AE 0.028
Starting Iteration 104
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 104: Reward 1.148 Loss 9.181 AE 0.028
Starting Iteration 105
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 105: Reward 1.135 Loss 8.236 AE 0.028
Starting Iteration 106
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 106: Reward 1.136 Loss 9.134 AE 0.028
Starting Iteration 107
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 107: Reward 1.158 Loss 8.909 AE 0.029
Starting Iteration 108
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 108: Reward 1.151 Loss 8.908 AE 0.028
Starting Iteration 109
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 109: Reward 1.185 Loss 9.567 AE 0.029
Starting Iteration 110
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 110: Reward 1.151 Loss 8.988 AE 0.028
Starting Iteration 111
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 111: Reward 1.155 Loss 9.488 AE 0.028
Starting Iteration 112
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 112: Reward 1.131 Loss 8.647 AE 0.028
Starting Iteration 113
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 113: Reward 1.164 Loss 9.001 AE 0.029
Starting Iteration 114
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 114: Reward 1.137 Loss 8.819 AE 0.029
Starting Iteration 115
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 115: Reward 1.150 Loss 9.955 AE 0.028
Starting Iteration 116
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 116: Reward 1.154 Loss 9.351 AE 0.028
Starting Iteration 117
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 117: Reward 1.187 Loss 9.774 AE 0.026
Starting Iteration 118
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 118: Reward 1.148 Loss 9.483 AE 0.026
Starting Iteration 119
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 119: Reward 1.175 Loss 10.159 AE 0.025
Starting Iteration 120
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 120: Reward 1.145 Loss 9.602 AE 0.025
Starting Iteration 121
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 121: Reward 1.157 Loss 11.271 AE 0.025
Starting Iteration 122
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 122: Reward 1.167 Loss 10.375 AE 0.026
Starting Iteration 123
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 123: Reward 1.157 Loss 9.099 AE 0.026
Starting Iteration 124
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 124: Reward 1.142 Loss 9.785 AE 0.027
Starting Iteration 125
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 125: Reward 1.128 Loss 10.070 AE 0.026
Starting Iteration 126
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 126: Reward 1.145 Loss 10.721 AE 0.027
Starting Iteration 127
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 127: Reward 1.175 Loss 12.090 AE 0.026
Starting Iteration 128
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 128: Reward 1.096 Loss 9.385 AE 0.026
Starting Iteration 129
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 129: Reward 1.129 Loss 8.818 AE 0.027
Starting Iteration 130
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 130: Reward 1.159 Loss 9.597 AE 0.026
Starting Iteration 131
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 131: Reward 1.165 Loss 10.250 AE 0.025
Starting Iteration 132
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 132: Reward 1.131 Loss 10.048 AE 0.025
Starting Iteration 133
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 133: Reward 1.158 Loss 9.879 AE 0.025
Starting Iteration 134
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 134: Reward 1.190 Loss 10.197 AE 0.025
Starting Iteration 135
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 135: Reward 1.220 Loss 10.550 AE 0.025
Starting Iteration 136
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 136: Reward 1.157 Loss 9.507 AE 0.025
Starting Iteration 137
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 137: Reward 1.144 Loss 10.001 AE 0.025
Starting Iteration 138
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 138: Reward 1.167 Loss 9.855 AE 0.026
Starting Iteration 139
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 139: Reward 1.186 Loss 9.905 AE 0.026
Starting Iteration 140
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 140: Reward 1.164 Loss 9.540 AE 0.027
Starting Iteration 141
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 141: Reward 1.157 Loss 9.042 AE 0.028
Starting Iteration 142
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 142: Reward 1.168 Loss 9.422 AE 0.027
Starting Iteration 143
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 143: Reward 1.163 Loss 10.201 AE 0.028
Starting Iteration 144
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 144: Reward 1.170 Loss 9.333 AE 0.028
Starting Iteration 145
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 145: Reward 1.161 Loss 9.126 AE 0.027
Starting Iteration 146
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 146: Reward 1.152 Loss 9.121 AE 0.027
Starting Iteration 147
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 147: Reward 1.167 Loss 9.002 AE 0.026
Starting Iteration 148
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 148: Reward 1.179 Loss 10.487 AE 0.025
Starting Iteration 149
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 149: Reward 1.156 Loss 9.645 AE 0.024
Starting Iteration 150
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 150: Reward 1.142 Loss 10.608 AE 0.024
Saved checkpoint to policy_150.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 151
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 151: Reward 1.141 Loss 10.560 AE 0.024
Starting Iteration 152
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 152: Reward 1.151 Loss 10.602 AE 0.023
Starting Iteration 153
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 153: Reward 1.125 Loss 10.633 AE 0.022
Starting Iteration 154
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 154: Reward 1.162 Loss 12.242 AE 0.022
Starting Iteration 155
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 155: Reward 1.162 Loss 12.766 AE 0.022
Starting Iteration 156
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 156: Reward 1.144 Loss 11.759 AE 0.022
Starting Iteration 157
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 157: Reward 1.145 Loss 10.735 AE 0.021
Starting Iteration 158
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 158: Reward 1.124 Loss 10.898 AE 0.022
Starting Iteration 159
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 159: Reward 1.128 Loss 10.191 AE 0.022
Starting Iteration 160
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 160: Reward 1.151 Loss 10.646 AE 0.022
Starting Iteration 161
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 161: Reward 1.130 Loss 10.751 AE 0.021
Starting Iteration 162
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 162: Reward 1.155 Loss 10.855 AE 0.021
Starting Iteration 163
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 163: Reward 1.140 Loss 11.956 AE 0.022
Starting Iteration 164
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 164: Reward 1.181 Loss 11.398 AE 0.022
Starting Iteration 165
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 165: Reward 1.153 Loss 9.897 AE 0.023
Starting Iteration 166
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 166: Reward 1.167 Loss 10.325 AE 0.024
Starting Iteration 167
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 167: Reward 1.144 Loss 9.887 AE 0.024
Starting Iteration 168
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 168: Reward 1.147 Loss 10.416 AE 0.026
Starting Iteration 169
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 169: Reward 1.142 Loss 9.370 AE 0.026
Starting Iteration 170
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 170: Reward 1.180 Loss 9.728 AE 0.026
Starting Iteration 171
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 171: Reward 1.129 Loss 9.279 AE 0.026
Starting Iteration 172
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 172: Reward 1.153 Loss 9.945 AE 0.026
Starting Iteration 173
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 173: Reward 1.171 Loss 9.702 AE 0.027
Starting Iteration 174
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 174: Reward 1.167 Loss 9.465 AE 0.027
Starting Iteration 175
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 175: Reward 1.202 Loss 11.005 AE 0.028
Starting Iteration 176
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 176: Reward 1.159 Loss 9.621 AE 0.029
Starting Iteration 177
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 177: Reward 1.157 Loss 9.900 AE 0.029
Starting Iteration 178
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 178: Reward 1.141 Loss 8.953 AE 0.027
Starting Iteration 179
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 179: Reward 1.170 Loss 9.794 AE 0.027
Starting Iteration 180
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 180: Reward 1.147 Loss 9.787 AE 0.026
Starting Iteration 181
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 181: Reward 1.160 Loss 10.752 AE 0.026
Starting Iteration 182
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 182: Reward 1.137 Loss 9.917 AE 0.026
Starting Iteration 183
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 183: Reward 1.179 Loss 10.312 AE 0.026
Starting Iteration 184
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 184: Reward 1.146 Loss 10.890 AE 0.025
Starting Iteration 185
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 185: Reward 1.176 Loss 10.163 AE 0.025
Starting Iteration 186
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 186: Reward 1.190 Loss 11.890 AE 0.024
Starting Iteration 187
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 187: Reward 1.184 Loss 11.602 AE 0.024
Starting Iteration 188
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 188: Reward 1.154 Loss 9.361 AE 0.025
Starting Iteration 189
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 189: Reward 1.165 Loss 10.162 AE 0.024
Starting Iteration 190
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 190: Reward 1.161 Loss 10.466 AE 0.025
Starting Iteration 191
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 191: Reward 1.166 Loss 9.975 AE 0.025
Starting Iteration 192
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 192: Reward 1.166 Loss 9.711 AE 0.025
Starting Iteration 193
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 193: Reward 1.143 Loss 9.164 AE 0.026
Starting Iteration 194
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 194: Reward 1.157 Loss 10.155 AE 0.026
Starting Iteration 195
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 195: Reward 1.170 Loss 9.470 AE 0.027
Starting Iteration 196
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 196: Reward 1.123 Loss 8.889 AE 0.029
Starting Iteration 197
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 197: Reward 1.099 Loss 8.173 AE 0.030
Starting Iteration 198
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 198: Reward 1.118 Loss 9.155 AE 0.030
Starting Iteration 199
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 199: Reward 1.130 Loss 9.255 AE 0.030
Starting Iteration 200
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 200: Reward 1.129 Loss 9.090 AE 0.030
Saved checkpoint to policy_200.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 201
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 201: Reward 1.105 Loss 8.884 AE 0.029
Starting Iteration 202
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 202: Reward 1.118 Loss 8.471 AE 0.029
Starting Iteration 203
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 203: Reward 1.109 Loss 8.705 AE 0.030
Starting Iteration 204
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 204: Reward 1.132 Loss 8.139 AE 0.029
Starting Iteration 205
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 205: Reward 1.129 Loss 8.687 AE 0.029
Starting Iteration 206
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 206: Reward 1.160 Loss 9.119 AE 0.028
Starting Iteration 207
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 207: Reward 1.129 Loss 7.973 AE 0.028
Starting Iteration 208
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 208: Reward 1.154 Loss 8.741 AE 0.028
Starting Iteration 209
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 209: Reward 1.191 Loss 9.450 AE 0.029
Starting Iteration 210
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 210: Reward 1.163 Loss 9.437 AE 0.030
Starting Iteration 211
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 211: Reward 1.154 Loss 9.495 AE 0.029
Starting Iteration 212
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 212: Reward 1.153 Loss 10.249 AE 0.030
Starting Iteration 213
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 213: Reward 1.162 Loss 10.345 AE 0.029
Starting Iteration 214
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 214: Reward 1.096 Loss 10.357 AE 0.027
Starting Iteration 215
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 215: Reward 1.139 Loss 10.337 AE 0.027
Starting Iteration 216
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 216: Reward 1.131 Loss 10.646 AE 0.026
Starting Iteration 217
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 217: Reward 1.158 Loss 11.404 AE 0.026
Starting Iteration 218
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 218: Reward 1.196 Loss 11.036 AE 0.025
Starting Iteration 219
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 219: Reward 1.156 Loss 10.574 AE 0.025
Starting Iteration 220
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 220: Reward 1.172 Loss 11.024 AE 0.026
Starting Iteration 221
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 221: Reward 1.151 Loss 11.269 AE 0.025
Starting Iteration 222
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 222: Reward 1.144 Loss 10.033 AE 0.025
Starting Iteration 223
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 223: Reward 1.172 Loss 10.574 AE 0.024
Starting Iteration 224
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 224: Reward 1.148 Loss 10.448 AE 0.024
Starting Iteration 225
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 225: Reward 1.170 Loss 11.447 AE 0.023
Starting Iteration 226
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 226: Reward 1.178 Loss 11.368 AE 0.023
Starting Iteration 227
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 227: Reward 1.164 Loss 9.769 AE 0.024
Starting Iteration 228
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 228: Reward 1.140 Loss 9.877 AE 0.023
Starting Iteration 229
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 229: Reward 1.183 Loss 10.688 AE 0.024
Starting Iteration 230
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 230: Reward 1.137 Loss 9.874 AE 0.025
Starting Iteration 231
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 231: Reward 1.167 Loss 9.386 AE 0.026
Starting Iteration 232
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 232: Reward 1.141 Loss 9.796 AE 0.025
Starting Iteration 233
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 233: Reward 1.183 Loss 10.453 AE 0.025
Starting Iteration 234
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 234: Reward 1.152 Loss 10.479 AE 0.026
Starting Iteration 235
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 235: Reward 1.117 Loss 9.464 AE 0.024
Starting Iteration 236
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 236: Reward 1.128 Loss 9.447 AE 0.024
Starting Iteration 237
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 237: Reward 1.146 Loss 10.299 AE 0.023
Starting Iteration 238
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 238: Reward 1.153 Loss 9.735 AE 0.023
Starting Iteration 239
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 239: Reward 1.158 Loss 9.753 AE 0.023
Starting Iteration 240
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 240: Reward 1.165 Loss 9.555 AE 0.025
Starting Iteration 241
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 241: Reward 1.146 Loss 10.198 AE 0.026
Starting Iteration 242
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 242: Reward 1.144 Loss 9.884 AE 0.025
Starting Iteration 243
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 243: Reward 1.132 Loss 9.766 AE 0.026
Starting Iteration 244
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 244: Reward 1.164 Loss 9.661 AE 0.026
Starting Iteration 245
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 245: Reward 1.153 Loss 10.514 AE 0.027
Starting Iteration 246
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 246: Reward 1.178 Loss 10.556 AE 0.026
Starting Iteration 247
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 247: Reward 1.174 Loss 10.197 AE 0.026
Starting Iteration 248
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 248: Reward 1.174 Loss 10.395 AE 0.026
Starting Iteration 249
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 249: Reward 1.187 Loss 10.116 AE 0.025
Starting Iteration 250
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 250: Reward 1.150 Loss 9.367 AE 0.025
Saved checkpoint to policy_250.pth
