WarpDrive not installed or not fully importable (likely missing pycuda). Using custom CPU trainer.
WARNING: CUDA or WarpDrive not available. Falling back to Custom CPU Training.
CPU Mode: Reduced agents to 200 and iterations to 1000
Starting Iteration 0
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 0: Reward 0.911 Loss 55.520 AE 0.180
Saved checkpoint to policy_0.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 1
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 1: Reward 0.919 Loss 53.130 AE 0.175
Starting Iteration 2
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 2: Reward 0.918 Loss 49.791 AE 0.170
Starting Iteration 3
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 3: Reward 0.939 Loss 50.349 AE 0.164
Starting Iteration 4
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 4: Reward 0.951 Loss 50.210 AE 0.161
Starting Iteration 5
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 5: Reward 0.928 Loss 46.682 AE 0.159
Starting Iteration 6
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 6: Reward 0.923 Loss 48.249 AE 0.156
Starting Iteration 7
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 7: Reward 0.899 Loss 42.778 AE 0.152
Starting Iteration 8
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 8: Reward 0.933 Loss 43.632 AE 0.147
Starting Iteration 9
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 9: Reward 0.937 Loss 45.473 AE 0.140
Starting Iteration 10
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 10: Reward 0.923 Loss 42.126 AE 0.132
Starting Iteration 11
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 11: Reward 0.929 Loss 43.709 AE 0.124
Starting Iteration 12
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 12: Reward 0.919 Loss 39.917 AE 0.118
Starting Iteration 13
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 13: Reward 0.923 Loss 40.224 AE 0.113
Starting Iteration 14
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 14: Reward 0.930 Loss 40.907 AE 0.110
Starting Iteration 15
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 15: Reward 0.892 Loss 36.965 AE 0.106
Starting Iteration 16
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 16: Reward 0.915 Loss 39.618 AE 0.102
Starting Iteration 17
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 17: Reward 0.909 Loss 35.453 AE 0.096
Starting Iteration 18
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 18: Reward 0.963 Loss 38.371 AE 0.092
Starting Iteration 19
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 19: Reward 0.966 Loss 34.769 AE 0.088
Starting Iteration 20
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 20: Reward 0.974 Loss 33.056 AE 0.084
Starting Iteration 21
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 21: Reward 0.994 Loss 35.894 AE 0.080
Starting Iteration 22
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 22: Reward 0.983 Loss 31.269 AE 0.076
Starting Iteration 23
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 23: Reward 0.981 Loss 37.596 AE 0.072
Starting Iteration 24
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 24: Reward 0.976 Loss 28.952 AE 0.068
Starting Iteration 25
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 25: Reward 0.985 Loss 28.626 AE 0.065
Starting Iteration 26
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 26: Reward 0.990 Loss 29.290 AE 0.061
Starting Iteration 27
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 27: Reward 1.007 Loss 31.828 AE 0.058
Starting Iteration 28
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 28: Reward 1.008 Loss 32.494 AE 0.055
Starting Iteration 29
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 29: Reward 0.981 Loss 33.448 AE 0.053
Starting Iteration 30
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 30: Reward 1.011 Loss 36.426 AE 0.052
Starting Iteration 31
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 31: Reward 1.021 Loss 36.542 AE 0.049
Starting Iteration 32
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 32: Reward 0.994 Loss 40.500 AE 0.049
Starting Iteration 33
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 33: Reward 1.016 Loss 39.316 AE 0.048
Starting Iteration 34
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 34: Reward 1.052 Loss 27.175 AE 0.046
Starting Iteration 35
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 35: Reward 1.022 Loss 27.875 AE 0.046
Starting Iteration 36
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 36: Reward 1.049 Loss 28.681 AE 0.045
Starting Iteration 37
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 37: Reward 1.060 Loss 26.128 AE 0.044
Starting Iteration 38
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 38: Reward 1.096 Loss 28.205 AE 0.043
Starting Iteration 39
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 39: Reward 1.048 Loss 21.557 AE 0.043
Starting Iteration 40
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 40: Reward 1.089 Loss 27.044 AE 0.044
Starting Iteration 41
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 41: Reward 1.083 Loss 27.409 AE 0.045
Starting Iteration 42
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 42: Reward 1.076 Loss 21.832 AE 0.045
Starting Iteration 43
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 43: Reward 1.044 Loss 21.459 AE 0.047
Starting Iteration 44
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 44: Reward 1.047 Loss 20.824 AE 0.046
Starting Iteration 45
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 45: Reward 1.074 Loss 22.099 AE 0.045
Starting Iteration 46
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 46: Reward 1.061 Loss 20.281 AE 0.044
Starting Iteration 47
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 47: Reward 1.069 Loss 20.472 AE 0.042
Starting Iteration 48
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 48: Reward 1.099 Loss 21.757 AE 0.042
Starting Iteration 49
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 49: Reward 1.074 Loss 21.325 AE 0.043
Starting Iteration 50
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 50: Reward 1.107 Loss 21.713 AE 0.043
Saved checkpoint to policy_50.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 51
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 51: Reward 1.084 Loss 20.958 AE 0.043
Starting Iteration 52
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 52: Reward 1.067 Loss 20.616 AE 0.043
Starting Iteration 53
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 53: Reward 1.066 Loss 20.238 AE 0.043
Starting Iteration 54
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 54: Reward 1.070 Loss 20.484 AE 0.044
Starting Iteration 55
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 55: Reward 1.085 Loss 19.981 AE 0.045
Starting Iteration 56
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 56: Reward 1.088 Loss 20.954 AE 0.045
Starting Iteration 57
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 57: Reward 1.062 Loss 20.011 AE 0.048
Starting Iteration 58
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 58: Reward 1.085 Loss 20.764 AE 0.047
Starting Iteration 59
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 59: Reward 1.042 Loss 19.326 AE 0.046
Starting Iteration 60
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 60: Reward 1.061 Loss 20.192 AE 0.046
Starting Iteration 61
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 61: Reward 1.082 Loss 19.911 AE 0.045
Starting Iteration 62
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 62: Reward 1.063 Loss 19.961 AE 0.044
Starting Iteration 63
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 63: Reward 1.132 Loss 22.793 AE 0.042
Starting Iteration 64
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 64: Reward 1.056 Loss 19.293 AE 0.042
Starting Iteration 65
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 65: Reward 1.062 Loss 19.651 AE 0.042
Starting Iteration 66
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 66: Reward 1.047 Loss 23.281 AE 0.043
Starting Iteration 67
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 67: Reward 1.025 Loss 18.866 AE 0.044
Starting Iteration 68
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 68: Reward 1.072 Loss 21.677 AE 0.044
Starting Iteration 69
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 69: Reward 1.023 Loss 19.152 AE 0.044
Starting Iteration 70
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 70: Reward 1.037 Loss 19.865 AE 0.044
Starting Iteration 71
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 71: Reward 1.053 Loss 21.371 AE 0.041
Starting Iteration 72
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 72: Reward 1.066 Loss 23.129 AE 0.040
Starting Iteration 73
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 73: Reward 1.086 Loss 20.325 AE 0.038
Starting Iteration 74
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 74: Reward 1.063 Loss 19.996 AE 0.037
Starting Iteration 75
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 75: Reward 1.014 Loss 18.663 AE 0.036
Starting Iteration 76
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 76: Reward 1.078 Loss 22.385 AE 0.036
Starting Iteration 77
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 77: Reward 1.019 Loss 19.455 AE 0.036
Starting Iteration 78
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 78: Reward 1.043 Loss 20.272 AE 0.036
Starting Iteration 79
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 79: Reward 1.067 Loss 20.131 AE 0.036
Starting Iteration 80
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 80: Reward 1.083 Loss 21.403 AE 0.036
Starting Iteration 81
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 81: Reward 1.061 Loss 20.799 AE 0.036
Starting Iteration 82
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 82: Reward 1.075 Loss 21.275 AE 0.037
Starting Iteration 83
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 83: Reward 1.102 Loss 21.105 AE 0.038
Starting Iteration 84
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 84: Reward 1.056 Loss 18.612 AE 0.040
Starting Iteration 85
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 85: Reward 1.077 Loss 20.264 AE 0.041
Starting Iteration 86
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 86: Reward 1.111 Loss 21.469 AE 0.041
Starting Iteration 87
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 87: Reward 1.093 Loss 21.117 AE 0.042
Starting Iteration 88
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 88: Reward 1.075 Loss 20.833 AE 0.042
Starting Iteration 89
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 89: Reward 1.059 Loss 19.793 AE 0.043
Starting Iteration 90
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 90: Reward 1.087 Loss 20.507 AE 0.041
Starting Iteration 91
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 91: Reward 1.069 Loss 19.473 AE 0.041
Starting Iteration 92
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 92: Reward 1.038 Loss 18.157 AE 0.041
Starting Iteration 93
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 93: Reward 1.083 Loss 19.887 AE 0.040
Starting Iteration 94
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 94: Reward 1.096 Loss 20.179 AE 0.039
Starting Iteration 95
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 95: Reward 1.088 Loss 20.378 AE 0.039
Starting Iteration 96
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 96: Reward 1.093 Loss 20.659 AE 0.039
Starting Iteration 97
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 97: Reward 1.093 Loss 19.626 AE 0.039
Starting Iteration 98
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 98: Reward 1.054 Loss 18.477 AE 0.039
Starting Iteration 99
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 99: Reward 1.065 Loss 20.196 AE 0.039
Starting Iteration 100
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 100: Reward 1.057 Loss 19.793 AE 0.040
Saved checkpoint to policy_100.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 101
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 101: Reward 1.079 Loss 20.695 AE 0.040
Starting Iteration 102
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 102: Reward 1.098 Loss 22.305 AE 0.040
Starting Iteration 103
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 103: Reward 1.026 Loss 18.885 AE 0.041
Starting Iteration 104
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 104: Reward 1.054 Loss 19.257 AE 0.040
Starting Iteration 105
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 105: Reward 1.051 Loss 21.039 AE 0.041
Starting Iteration 106
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 106: Reward 1.051 Loss 19.605 AE 0.040
Starting Iteration 107
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 107: Reward 1.057 Loss 19.863 AE 0.039
Starting Iteration 108
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 108: Reward 1.045 Loss 19.619 AE 0.039
Starting Iteration 109
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 109: Reward 1.083 Loss 20.612 AE 0.037
Starting Iteration 110
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 110: Reward 1.053 Loss 19.043 AE 0.036
Starting Iteration 111
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 111: Reward 1.072 Loss 19.902 AE 0.033
Starting Iteration 112
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 112: Reward 1.131 Loss 22.304 AE 0.030
Starting Iteration 113
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 113: Reward 1.141 Loss 21.852 AE 0.028
Starting Iteration 114
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 114: Reward 1.122 Loss 21.579 AE 0.026
Starting Iteration 115
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 115: Reward 1.083 Loss 21.458 AE 0.025
Starting Iteration 116
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 116: Reward 1.072 Loss 21.291 AE 0.026
Starting Iteration 117
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 117: Reward 1.110 Loss 22.340 AE 0.024
Starting Iteration 118
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 118: Reward 1.044 Loss 20.852 AE 0.025
Starting Iteration 119
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 119: Reward 1.040 Loss 20.803 AE 0.025
Starting Iteration 120
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 120: Reward 1.053 Loss 19.966 AE 0.026
Starting Iteration 121
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 121: Reward 1.051 Loss 21.310 AE 0.028
Starting Iteration 122
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 122: Reward 1.095 Loss 22.249 AE 0.028
Starting Iteration 123
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 123: Reward 1.065 Loss 21.186 AE 0.030
Starting Iteration 124
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 124: Reward 1.096 Loss 22.844 AE 0.030
Starting Iteration 125
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 125: Reward 1.140 Loss 25.331 AE 0.030
Starting Iteration 126
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 126: Reward 1.086 Loss 21.871 AE 0.032
Starting Iteration 127
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 127: Reward 1.136 Loss 25.526 AE 0.033
Starting Iteration 128
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 128: Reward 1.107 Loss 22.103 AE 0.033
Starting Iteration 129
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 129: Reward 1.120 Loss 22.545 AE 0.033
Starting Iteration 130
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 130: Reward 1.078 Loss 20.387 AE 0.034
Starting Iteration 131
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 131: Reward 1.085 Loss 19.850 AE 0.035
Starting Iteration 132
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 132: Reward 1.093 Loss 20.584 AE 0.035
Starting Iteration 133
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 133: Reward 1.114 Loss 21.925 AE 0.034
Starting Iteration 134
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 134: Reward 1.135 Loss 22.667 AE 0.033
Starting Iteration 135
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 135: Reward 1.137 Loss 20.961 AE 0.032
Starting Iteration 136
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 136: Reward 1.114 Loss 20.841 AE 0.032
Starting Iteration 137
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 137: Reward 1.111 Loss 19.791 AE 0.031
Starting Iteration 138
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 138: Reward 1.111 Loss 21.375 AE 0.031
Starting Iteration 139
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 139: Reward 1.068 Loss 20.865 AE 0.031
Starting Iteration 140
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 140: Reward 1.058 Loss 21.003 AE 0.032
Starting Iteration 141
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 141: Reward 1.044 Loss 21.939 AE 0.032
Starting Iteration 142
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 142: Reward 1.068 Loss 20.355 AE 0.032
Starting Iteration 143
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 143: Reward 1.057 Loss 20.604 AE 0.032
Starting Iteration 144
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 144: Reward 1.037 Loss 18.233 AE 0.031
Starting Iteration 145
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 145: Reward 1.091 Loss 21.591 AE 0.030
Starting Iteration 146
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 146: Reward 1.083 Loss 21.426 AE 0.030
Starting Iteration 147
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 147: Reward 1.080 Loss 20.989 AE 0.030
Starting Iteration 148
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 148: Reward 1.107 Loss 21.514 AE 0.030
Starting Iteration 149
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 149: Reward 1.096 Loss 22.060 AE 0.029
Starting Iteration 150
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 150: Reward 1.092 Loss 20.617 AE 0.029
Saved checkpoint to policy_150.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 151
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 151: Reward 1.078 Loss 19.867 AE 0.029
Starting Iteration 152
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 152: Reward 1.073 Loss 19.902 AE 0.029
Starting Iteration 153
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 153: Reward 1.097 Loss 20.999 AE 0.028
Starting Iteration 154
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 154: Reward 1.102 Loss 21.686 AE 0.026
Starting Iteration 155
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 155: Reward 1.061 Loss 20.885 AE 0.026
Starting Iteration 156
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 156: Reward 1.071 Loss 21.346 AE 0.025
Starting Iteration 157
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 157: Reward 1.089 Loss 19.891 AE 0.026
Starting Iteration 158
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 158: Reward 1.101 Loss 20.303 AE 0.026
Starting Iteration 159
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 159: Reward 1.104 Loss 21.414 AE 0.027
Starting Iteration 160
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 160: Reward 1.127 Loss 21.326 AE 0.028
Starting Iteration 161
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 161: Reward 1.120 Loss 20.095 AE 0.028
Starting Iteration 162
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 162: Reward 1.144 Loss 21.926 AE 0.029
Starting Iteration 163
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 163: Reward 1.085 Loss 19.546 AE 0.030
Starting Iteration 164
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 164: Reward 1.106 Loss 18.799 AE 0.030
Starting Iteration 165
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 165: Reward 1.091 Loss 17.665 AE 0.030
Starting Iteration 166
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 166: Reward 1.100 Loss 19.128 AE 0.030
Starting Iteration 167
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 167: Reward 1.096 Loss 19.570 AE 0.030
Starting Iteration 168
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 168: Reward 1.082 Loss 18.246 AE 0.029
Starting Iteration 169
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 169: Reward 1.115 Loss 18.795 AE 0.028
Starting Iteration 170
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 170: Reward 1.125 Loss 20.285 AE 0.029
Starting Iteration 171
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 171: Reward 1.128 Loss 19.286 AE 0.029
Starting Iteration 172
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 172: Reward 1.095 Loss 18.961 AE 0.030
Starting Iteration 173
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 173: Reward 1.147 Loss 20.851 AE 0.029
Starting Iteration 174
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 174: Reward 1.120 Loss 19.358 AE 0.028
Starting Iteration 175
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 175: Reward 1.129 Loss 20.529 AE 0.028
Starting Iteration 176
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 176: Reward 1.128 Loss 20.078 AE 0.029
Starting Iteration 177
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 177: Reward 1.101 Loss 18.841 AE 0.028
Starting Iteration 178
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 178: Reward 1.124 Loss 19.531 AE 0.028
Starting Iteration 179
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 179: Reward 1.112 Loss 19.986 AE 0.028
Starting Iteration 180
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 180: Reward 1.135 Loss 20.839 AE 0.028
Starting Iteration 181
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 181: Reward 1.123 Loss 19.498 AE 0.030
Starting Iteration 182
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 182: Reward 1.109 Loss 19.249 AE 0.030
Starting Iteration 183
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 183: Reward 1.120 Loss 19.514 AE 0.031
Starting Iteration 184
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 184: Reward 1.119 Loss 18.681 AE 0.031
Starting Iteration 185
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 185: Reward 1.104 Loss 19.234 AE 0.031
Starting Iteration 186
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 186: Reward 1.122 Loss 19.469 AE 0.031
Starting Iteration 187
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 187: Reward 1.111 Loss 19.079 AE 0.032
Starting Iteration 188
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 188: Reward 1.082 Loss 18.328 AE 0.032
Starting Iteration 189
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 189: Reward 1.132 Loss 20.845 AE 0.033
Starting Iteration 190
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 190: Reward 1.092 Loss 17.831 AE 0.033
Starting Iteration 191
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 191: Reward 1.105 Loss 18.986 AE 0.035
Starting Iteration 192
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 192: Reward 1.084 Loss 16.623 AE 0.035
Starting Iteration 193
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 193: Reward 1.127 Loss 20.304 AE 0.035
Starting Iteration 194
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 194: Reward 1.090 Loss 18.533 AE 0.035
Starting Iteration 195
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 195: Reward 1.104 Loss 19.390 AE 0.034
Starting Iteration 196
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 196: Reward 1.104 Loss 18.551 AE 0.035
Starting Iteration 197
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 197: Reward 1.114 Loss 19.445 AE 0.035
Starting Iteration 198
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 198: Reward 1.079 Loss 18.274 AE 0.035
Starting Iteration 199
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 199: Reward 1.097 Loss 19.375 AE 0.035
Starting Iteration 200
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 200: Reward 1.091 Loss 18.269 AE 0.035
Saved checkpoint to policy_200.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 201
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 201: Reward 1.072 Loss 17.236 AE 0.035
Starting Iteration 202
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 202: Reward 1.102 Loss 19.345 AE 0.036
Starting Iteration 203
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 203: Reward 1.097 Loss 18.945 AE 0.036
Starting Iteration 204
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 204: Reward 1.125 Loss 19.251 AE 0.035
Starting Iteration 205
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 205: Reward 1.085 Loss 18.127 AE 0.035
Starting Iteration 206
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 206: Reward 1.070 Loss 16.047 AE 0.033
Starting Iteration 207
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 207: Reward 1.079 Loss 17.378 AE 0.034
Starting Iteration 208
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 208: Reward 1.081 Loss 17.013 AE 0.033
Starting Iteration 209
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 209: Reward 1.102 Loss 17.479 AE 0.033
Starting Iteration 210
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 210: Reward 1.111 Loss 18.414 AE 0.033
Starting Iteration 211
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 211: Reward 1.083 Loss 17.513 AE 0.033
Starting Iteration 212
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 212: Reward 1.081 Loss 17.609 AE 0.033
Starting Iteration 213
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 213: Reward 1.072 Loss 18.357 AE 0.033
Starting Iteration 214
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 214: Reward 1.072 Loss 18.465 AE 0.033
Starting Iteration 215
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 215: Reward 1.079 Loss 18.491 AE 0.033
Starting Iteration 216
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 216: Reward 1.062 Loss 19.650 AE 0.033
Starting Iteration 217
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 217: Reward 1.069 Loss 18.104 AE 0.033
Starting Iteration 218
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 218: Reward 1.069 Loss 18.789 AE 0.034
Starting Iteration 219
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 219: Reward 1.058 Loss 18.476 AE 0.033
Starting Iteration 220
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 220: Reward 1.059 Loss 17.307 AE 0.033
Starting Iteration 221
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 221: Reward 1.081 Loss 18.505 AE 0.032
Starting Iteration 222
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 222: Reward 1.140 Loss 20.281 AE 0.032
Starting Iteration 223
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 223: Reward 1.111 Loss 19.764 AE 0.032
Starting Iteration 224
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 224: Reward 1.103 Loss 18.475 AE 0.031
Starting Iteration 225
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 225: Reward 1.091 Loss 17.765 AE 0.032
Starting Iteration 226
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 226: Reward 1.093 Loss 18.138 AE 0.031
Starting Iteration 227
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 227: Reward 1.108 Loss 17.206 AE 0.031
Starting Iteration 228
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 228: Reward 1.110 Loss 17.075 AE 0.030
Starting Iteration 229
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 229: Reward 1.095 Loss 17.757 AE 0.030
Starting Iteration 230
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 230: Reward 1.067 Loss 17.432 AE 0.030
Starting Iteration 231
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 231: Reward 1.122 Loss 19.435 AE 0.030
Starting Iteration 232
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 232: Reward 1.093 Loss 19.316 AE 0.030
Starting Iteration 233
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 233: Reward 1.115 Loss 20.627 AE 0.030
Starting Iteration 234
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 234: Reward 1.098 Loss 18.271 AE 0.029
Starting Iteration 235
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 235: Reward 1.082 Loss 16.765 AE 0.030
Starting Iteration 236
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 236: Reward 1.093 Loss 16.501 AE 0.030
Starting Iteration 237
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 237: Reward 1.124 Loss 18.043 AE 0.030
Starting Iteration 238
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 238: Reward 1.092 Loss 17.430 AE 0.030
Starting Iteration 239
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 239: Reward 1.093 Loss 16.589 AE 0.029
Starting Iteration 240
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 240: Reward 1.133 Loss 18.107 AE 0.029
Starting Iteration 241
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 241: Reward 1.148 Loss 18.244 AE 0.028
Starting Iteration 242
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 242: Reward 1.154 Loss 18.473 AE 0.027
Starting Iteration 243
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 243: Reward 1.149 Loss 19.040 AE 0.027
Starting Iteration 244
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 244: Reward 1.123 Loss 16.738 AE 0.026
Starting Iteration 245
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 245: Reward 1.124 Loss 18.048 AE 0.027
Starting Iteration 246
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 246: Reward 1.130 Loss 18.435 AE 0.027
Starting Iteration 247
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 247: Reward 1.109 Loss 18.246 AE 0.027
Starting Iteration 248
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 248: Reward 1.131 Loss 17.905 AE 0.026
Starting Iteration 249
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 249: Reward 1.147 Loss 19.742 AE 0.026
Starting Iteration 250
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 250: Reward 1.134 Loss 18.660 AE 0.027
Saved checkpoint to policy_250.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 251
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 251: Reward 1.092 Loss 17.207 AE 0.026
Starting Iteration 252
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 252: Reward 1.123 Loss 19.432 AE 0.025
Starting Iteration 253
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 253: Reward 1.088 Loss 17.984 AE 0.025
Starting Iteration 254
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 254: Reward 1.100 Loss 17.578 AE 0.023
Starting Iteration 255
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 255: Reward 1.130 Loss 18.965 AE 0.023
Starting Iteration 256
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 256: Reward 1.076 Loss 18.483 AE 0.022
Starting Iteration 257
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 257: Reward 1.089 Loss 21.432 AE 0.022
Starting Iteration 258
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 258: Reward 1.072 Loss 20.489 AE 0.021
Starting Iteration 259
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 259: Reward 1.050 Loss 19.898 AE 0.022
Starting Iteration 260
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 260: Reward 1.031 Loss 18.570 AE 0.021
Starting Iteration 261
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 261: Reward 1.018 Loss 17.434 AE 0.022
Starting Iteration 262
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 262: Reward 1.099 Loss 20.668 AE 0.021
Starting Iteration 263
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 263: Reward 1.108 Loss 20.519 AE 0.021
Starting Iteration 264
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 264: Reward 1.068 Loss 17.414 AE 0.022
Starting Iteration 265
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 265: Reward 1.089 Loss 19.603 AE 0.021
Starting Iteration 266
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 266: Reward 1.074 Loss 17.572 AE 0.022
Starting Iteration 267
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 267: Reward 1.106 Loss 19.450 AE 0.023
Starting Iteration 268
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 268: Reward 1.070 Loss 16.844 AE 0.024
Starting Iteration 269
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 269: Reward 1.087 Loss 17.568 AE 0.025
Starting Iteration 270
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 270: Reward 1.081 Loss 18.190 AE 0.025
Starting Iteration 271
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 271: Reward 1.064 Loss 17.679 AE 0.025
Starting Iteration 272
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 272: Reward 1.085 Loss 16.768 AE 0.026
Starting Iteration 273
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 273: Reward 1.116 Loss 18.017 AE 0.027
Starting Iteration 274
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 274: Reward 1.152 Loss 18.782 AE 0.027
Starting Iteration 275
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 275: Reward 1.117 Loss 17.317 AE 0.027
Starting Iteration 276
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 276: Reward 1.125 Loss 17.594 AE 0.029
Starting Iteration 277
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 277: Reward 1.118 Loss 16.338 AE 0.030
Starting Iteration 278
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 278: Reward 1.135 Loss 16.418 AE 0.031
Starting Iteration 279
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 279: Reward 1.140 Loss 17.892 AE 0.033
Starting Iteration 280
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 280: Reward 1.110 Loss 16.414 AE 0.033
Starting Iteration 281
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 281: Reward 1.086 Loss 17.053 AE 0.034
Starting Iteration 282
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 282: Reward 1.098 Loss 16.388 AE 0.034
Starting Iteration 283
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 283: Reward 1.093 Loss 17.467 AE 0.035
Starting Iteration 284
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 284: Reward 1.121 Loss 17.285 AE 0.034
Starting Iteration 285
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 285: Reward 1.109 Loss 16.647 AE 0.034
Starting Iteration 286
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 286: Reward 1.106 Loss 16.989 AE 0.034
Starting Iteration 287
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 287: Reward 1.097 Loss 16.218 AE 0.035
Starting Iteration 288
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 288: Reward 1.084 Loss 16.507 AE 0.035
Starting Iteration 289
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 289: Reward 1.147 Loss 18.784 AE 0.033
Starting Iteration 290
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 290: Reward 1.088 Loss 15.744 AE 0.034
Starting Iteration 291
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 291: Reward 1.079 Loss 16.758 AE 0.034
Starting Iteration 292
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 292: Reward 1.107 Loss 16.344 AE 0.034
Starting Iteration 293
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 293: Reward 1.108 Loss 16.448 AE 0.033
Starting Iteration 294
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 294: Reward 1.089 Loss 15.568 AE 0.033
Starting Iteration 295
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 295: Reward 1.123 Loss 17.278 AE 0.033
Starting Iteration 296
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 296: Reward 1.104 Loss 15.609 AE 0.033
Starting Iteration 297
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 297: Reward 1.121 Loss 15.745 AE 0.032
Starting Iteration 298
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 298: Reward 1.107 Loss 16.368 AE 0.033
Starting Iteration 299
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 299: Reward 1.107 Loss 17.255 AE 0.033
Starting Iteration 300
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 300: Reward 1.101 Loss 16.244 AE 0.033
Saved checkpoint to policy_300.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 301
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 301: Reward 1.066 Loss 16.185 AE 0.033
Starting Iteration 302
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 302: Reward 1.106 Loss 15.762 AE 0.032
Starting Iteration 303
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 303: Reward 1.122 Loss 16.228 AE 0.032
Starting Iteration 304
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 304: Reward 1.085 Loss 15.100 AE 0.033
Starting Iteration 305
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 305: Reward 1.091 Loss 16.202 AE 0.033
Starting Iteration 306
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 306: Reward 1.110 Loss 16.949 AE 0.032
Starting Iteration 307
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 307: Reward 1.136 Loss 17.343 AE 0.031
Starting Iteration 308
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 308: Reward 1.088 Loss 16.881 AE 0.032
Starting Iteration 309
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 309: Reward 1.114 Loss 17.136 AE 0.031
Starting Iteration 310
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 310: Reward 1.130 Loss 17.284 AE 0.031
Starting Iteration 311
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 311: Reward 1.123 Loss 17.538 AE 0.030
Starting Iteration 312
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 312: Reward 1.112 Loss 15.952 AE 0.029
Starting Iteration 313
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 313: Reward 1.143 Loss 17.385 AE 0.029
Starting Iteration 314
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 314: Reward 1.129 Loss 16.245 AE 0.029
Starting Iteration 315
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 315: Reward 1.130 Loss 15.532 AE 0.028
Starting Iteration 316
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 316: Reward 1.109 Loss 15.031 AE 0.027
Starting Iteration 317
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 317: Reward 1.141 Loss 17.373 AE 0.026
Starting Iteration 318
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 318: Reward 1.116 Loss 17.996 AE 0.026
Starting Iteration 319
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 319: Reward 1.136 Loss 16.450 AE 0.026
Starting Iteration 320
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 320: Reward 1.153 Loss 18.263 AE 0.025
Starting Iteration 321
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 321: Reward 1.120 Loss 16.977 AE 0.025
Starting Iteration 322
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 322: Reward 1.116 Loss 16.745 AE 0.025
Starting Iteration 323
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 323: Reward 1.127 Loss 16.526 AE 0.025
Starting Iteration 324
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 324: Reward 1.069 Loss 15.885 AE 0.025
Starting Iteration 325
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 325: Reward 1.133 Loss 18.241 AE 0.024
Starting Iteration 326
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 326: Reward 1.158 Loss 19.405 AE 0.023
Starting Iteration 327
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 327: Reward 1.113 Loss 17.120 AE 0.023
Starting Iteration 328
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 328: Reward 1.115 Loss 16.952 AE 0.022
Starting Iteration 329
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 329: Reward 1.116 Loss 17.181 AE 0.023
Starting Iteration 330
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 330: Reward 1.103 Loss 16.033 AE 0.022
Starting Iteration 331
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 331: Reward 1.093 Loss 16.642 AE 0.022
Starting Iteration 332
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 332: Reward 1.114 Loss 16.529 AE 0.022
Starting Iteration 333
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 333: Reward 1.146 Loss 17.343 AE 0.022
Starting Iteration 334
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 334: Reward 1.126 Loss 15.410 AE 0.022
Starting Iteration 335
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 335: Reward 1.106 Loss 15.692 AE 0.022
Starting Iteration 336
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 336: Reward 1.149 Loss 15.764 AE 0.023
Starting Iteration 337
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 337: Reward 1.152 Loss 15.479 AE 0.024
Starting Iteration 338
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 338: Reward 1.146 Loss 14.920 AE 0.024
Starting Iteration 339
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 339: Reward 1.136 Loss 16.010 AE 0.026
Starting Iteration 340
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 340: Reward 1.136 Loss 16.342 AE 0.026
Starting Iteration 341
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 341: Reward 1.160 Loss 16.377 AE 0.026
Starting Iteration 342
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 342: Reward 1.136 Loss 15.872 AE 0.027
Starting Iteration 343
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 343: Reward 1.138 Loss 16.093 AE 0.027
Starting Iteration 344
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 344: Reward 1.123 Loss 14.405 AE 0.027
Starting Iteration 345
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 345: Reward 1.124 Loss 15.003 AE 0.026
Starting Iteration 346
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 346: Reward 1.136 Loss 14.801 AE 0.026
Starting Iteration 347
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 347: Reward 1.144 Loss 16.445 AE 0.026
Starting Iteration 348
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 348: Reward 1.128 Loss 15.736 AE 0.026
Starting Iteration 349
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 349: Reward 1.146 Loss 16.308 AE 0.026
Starting Iteration 350
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 350: Reward 1.078 Loss 14.768 AE 0.026
Saved checkpoint to policy_350.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 351
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 351: Reward 1.099 Loss 14.349 AE 0.026
Starting Iteration 352
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 352: Reward 1.148 Loss 15.491 AE 0.026
Starting Iteration 353
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 353: Reward 1.136 Loss 16.273 AE 0.027
Starting Iteration 354
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 354: Reward 1.116 Loss 15.558 AE 0.027
Starting Iteration 355
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 355: Reward 1.120 Loss 15.280 AE 0.028
Starting Iteration 356
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 356: Reward 1.118 Loss 14.504 AE 0.028
Starting Iteration 357
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 357: Reward 1.126 Loss 15.351 AE 0.028
Starting Iteration 358
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 358: Reward 1.124 Loss 14.377 AE 0.028
Starting Iteration 359
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 359: Reward 1.135 Loss 14.778 AE 0.028
Starting Iteration 360
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 360: Reward 1.143 Loss 14.372 AE 0.029
Starting Iteration 361
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 361: Reward 1.164 Loss 15.263 AE 0.028
Starting Iteration 362
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 362: Reward 1.139 Loss 14.427 AE 0.028
Starting Iteration 363
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 363: Reward 1.106 Loss 13.974 AE 0.028
Starting Iteration 364
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 364: Reward 1.116 Loss 15.374 AE 0.028
Starting Iteration 365
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 365: Reward 1.127 Loss 14.629 AE 0.028
Starting Iteration 366
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 366: Reward 1.147 Loss 14.858 AE 0.027
Starting Iteration 367
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 367: Reward 1.122 Loss 14.899 AE 0.027
Starting Iteration 368
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 368: Reward 1.143 Loss 15.541 AE 0.026
Starting Iteration 369
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 369: Reward 1.138 Loss 14.429 AE 0.027
Starting Iteration 370
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 370: Reward 1.133 Loss 13.818 AE 0.026
Starting Iteration 371
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 371: Reward 1.153 Loss 15.744 AE 0.026
Starting Iteration 372
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 372: Reward 1.135 Loss 13.753 AE 0.025
Starting Iteration 373
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 373: Reward 1.145 Loss 16.055 AE 0.024
Starting Iteration 374
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 374: Reward 1.128 Loss 14.596 AE 0.024
Starting Iteration 375
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 375: Reward 1.129 Loss 14.087 AE 0.023
Starting Iteration 376
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 376: Reward 1.137 Loss 14.515 AE 0.023
Starting Iteration 377
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 377: Reward 1.145 Loss 15.766 AE 0.023
Starting Iteration 378
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 378: Reward 1.138 Loss 15.634 AE 0.023
Starting Iteration 379
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 379: Reward 1.121 Loss 13.298 AE 0.023
Starting Iteration 380
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 380: Reward 1.125 Loss 15.233 AE 0.022
Starting Iteration 381
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 381: Reward 1.138 Loss 15.352 AE 0.022
Starting Iteration 382
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 382: Reward 1.126 Loss 13.409 AE 0.022
Starting Iteration 383
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 383: Reward 1.179 Loss 17.436 AE 0.022
Starting Iteration 384
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 384: Reward 1.176 Loss 15.672 AE 0.022
Starting Iteration 385
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 385: Reward 1.136 Loss 14.141 AE 0.024
Starting Iteration 386
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 386: Reward 1.163 Loss 13.689 AE 0.024
Starting Iteration 387
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 387: Reward 1.155 Loss 14.650 AE 0.025
Starting Iteration 388
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 388: Reward 1.142 Loss 14.146 AE 0.025
Starting Iteration 389
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 389: Reward 1.151 Loss 13.741 AE 0.026
Starting Iteration 390
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 390: Reward 1.120 Loss 13.129 AE 0.026
Starting Iteration 391
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 391: Reward 1.148 Loss 14.262 AE 0.026
Starting Iteration 392
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 392: Reward 1.131 Loss 14.042 AE 0.027
Starting Iteration 393
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 393: Reward 1.119 Loss 15.175 AE 0.028
Starting Iteration 394
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 394: Reward 1.104 Loss 14.514 AE 0.029
Starting Iteration 395
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 395: Reward 1.147 Loss 15.187 AE 0.029
Starting Iteration 396
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 396: Reward 1.104 Loss 13.234 AE 0.029
Starting Iteration 397
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 397: Reward 1.120 Loss 14.721 AE 0.029
Starting Iteration 398
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 398: Reward 1.149 Loss 15.346 AE 0.029
Starting Iteration 399
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 399: Reward 1.088 Loss 13.015 AE 0.029
Starting Iteration 400
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 400: Reward 1.155 Loss 14.124 AE 0.028
Saved checkpoint to policy_400.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 401
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 401: Reward 1.155 Loss 15.644 AE 0.028
Starting Iteration 402
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 402: Reward 1.107 Loss 13.557 AE 0.028
Starting Iteration 403
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 403: Reward 1.144 Loss 15.689 AE 0.027
Starting Iteration 404
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 404: Reward 1.174 Loss 16.635 AE 0.027
Starting Iteration 405
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 405: Reward 1.133 Loss 14.561 AE 0.026
Starting Iteration 406
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 406: Reward 1.164 Loss 15.215 AE 0.026
Starting Iteration 407
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 407: Reward 1.139 Loss 15.089 AE 0.025
Starting Iteration 408
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 408: Reward 1.171 Loss 15.373 AE 0.024
Starting Iteration 409
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 409: Reward 1.153 Loss 14.023 AE 0.024
Starting Iteration 410
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 410: Reward 1.145 Loss 16.271 AE 0.023
Starting Iteration 411
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 411: Reward 1.185 Loss 15.440 AE 0.022
Starting Iteration 412
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 412: Reward 1.171 Loss 15.163 AE 0.022
Starting Iteration 413
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 413: Reward 1.143 Loss 14.544 AE 0.023
Starting Iteration 414
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 414: Reward 1.131 Loss 14.757 AE 0.022
Starting Iteration 415
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 415: Reward 1.124 Loss 14.613 AE 0.022
Starting Iteration 416
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 416: Reward 1.152 Loss 15.326 AE 0.022
Starting Iteration 417
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 417: Reward 1.117 Loss 14.136 AE 0.022
Starting Iteration 418
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 418: Reward 1.117 Loss 13.399 AE 0.023
Starting Iteration 419
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 419: Reward 1.131 Loss 13.755 AE 0.024
Starting Iteration 420
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 420: Reward 1.126 Loss 13.045 AE 0.025
Starting Iteration 421
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 421: Reward 1.161 Loss 13.087 AE 0.026
Starting Iteration 422
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 422: Reward 1.163 Loss 13.576 AE 0.026
Starting Iteration 423
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 423: Reward 1.136 Loss 12.908 AE 0.026
Starting Iteration 424
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 424: Reward 1.132 Loss 13.429 AE 0.027
Starting Iteration 425
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 425: Reward 1.142 Loss 12.314 AE 0.027
Starting Iteration 426
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 426: Reward 1.150 Loss 13.475 AE 0.026
Starting Iteration 427
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 427: Reward 1.129 Loss 12.477 AE 0.025
Starting Iteration 428
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 428: Reward 1.175 Loss 13.970 AE 0.027
Starting Iteration 429
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 429: Reward 1.126 Loss 12.901 AE 0.027
Starting Iteration 430
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 430: Reward 1.145 Loss 13.077 AE 0.026
Starting Iteration 431
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 431: Reward 1.149 Loss 13.891 AE 0.026
Starting Iteration 432
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 432: Reward 1.129 Loss 13.536 AE 0.025
Starting Iteration 433
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 433: Reward 1.134 Loss 14.805 AE 0.025
Starting Iteration 434
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 434: Reward 1.130 Loss 14.459 AE 0.025
Starting Iteration 435
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 435: Reward 1.142 Loss 14.693 AE 0.025
Starting Iteration 436
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 436: Reward 1.134 Loss 13.291 AE 0.025
Starting Iteration 437
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 437: Reward 1.154 Loss 14.366 AE 0.025
Starting Iteration 438
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 438: Reward 1.151 Loss 13.659 AE 0.024
Starting Iteration 439
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 439: Reward 1.156 Loss 13.864 AE 0.025
Starting Iteration 440
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 440: Reward 1.177 Loss 13.983 AE 0.025
Starting Iteration 441
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 441: Reward 1.146 Loss 14.282 AE 0.024
Starting Iteration 442
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 442: Reward 1.166 Loss 14.597 AE 0.023
Starting Iteration 443
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 443: Reward 1.102 Loss 12.550 AE 0.024
Starting Iteration 444
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 444: Reward 1.145 Loss 14.194 AE 0.023
Starting Iteration 445
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 445: Reward 1.138 Loss 13.517 AE 0.023
Starting Iteration 446
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 446: Reward 1.126 Loss 13.669 AE 0.022
Starting Iteration 447
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 447: Reward 1.159 Loss 13.796 AE 0.022
Starting Iteration 448
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 448: Reward 1.145 Loss 13.827 AE 0.023
Starting Iteration 449
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 449: Reward 1.196 Loss 14.514 AE 0.023
Starting Iteration 450
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 450: Reward 1.159 Loss 14.368 AE 0.023
Saved checkpoint to policy_450.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 451
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 451: Reward 1.168 Loss 14.013 AE 0.023
Starting Iteration 452
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 452: Reward 1.164 Loss 13.993 AE 0.023
Starting Iteration 453
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 453: Reward 1.135 Loss 13.387 AE 0.023
Starting Iteration 454
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 454: Reward 1.141 Loss 12.908 AE 0.022
Starting Iteration 455
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 455: Reward 1.135 Loss 13.137 AE 0.022
Starting Iteration 456
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 456: Reward 1.110 Loss 13.011 AE 0.023
Starting Iteration 457
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 457: Reward 1.141 Loss 14.586 AE 0.023
Starting Iteration 458
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 458: Reward 1.132 Loss 12.444 AE 0.024
Starting Iteration 459
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 459: Reward 1.144 Loss 12.689 AE 0.024
Starting Iteration 460
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 460: Reward 1.131 Loss 12.702 AE 0.026
Starting Iteration 461
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 461: Reward 1.171 Loss 13.296 AE 0.027
Starting Iteration 462
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 462: Reward 1.173 Loss 12.714 AE 0.028
Starting Iteration 463
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 463: Reward 1.117 Loss 11.464 AE 0.027
Starting Iteration 464
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 464: Reward 1.148 Loss 11.515 AE 0.027
Starting Iteration 465
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 465: Reward 1.161 Loss 11.949 AE 0.026
Starting Iteration 466
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 466: Reward 1.135 Loss 11.475 AE 0.026
Starting Iteration 467
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 467: Reward 1.151 Loss 11.947 AE 0.027
Starting Iteration 468
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 468: Reward 1.153 Loss 11.493 AE 0.027
Starting Iteration 469
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 469: Reward 1.132 Loss 11.351 AE 0.028
Starting Iteration 470
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 470: Reward 1.140 Loss 11.722 AE 0.029
Starting Iteration 471
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 471: Reward 1.105 Loss 10.874 AE 0.028
Starting Iteration 472
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 472: Reward 1.151 Loss 12.401 AE 0.028
Starting Iteration 473
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 473: Reward 1.176 Loss 12.569 AE 0.028
Starting Iteration 474
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 474: Reward 1.158 Loss 12.646 AE 0.028
Starting Iteration 475
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 475: Reward 1.123 Loss 11.486 AE 0.028
Starting Iteration 476
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 476: Reward 1.157 Loss 12.998 AE 0.028
Starting Iteration 477
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 477: Reward 1.136 Loss 11.035 AE 0.027
Starting Iteration 478
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 478: Reward 1.148 Loss 12.792 AE 0.027
Starting Iteration 479
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 479: Reward 1.147 Loss 12.001 AE 0.026
Starting Iteration 480
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 480: Reward 1.150 Loss 12.065 AE 0.026
Starting Iteration 481
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 481: Reward 1.150 Loss 11.558 AE 0.026
Starting Iteration 482
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 482: Reward 1.137 Loss 11.958 AE 0.026
Starting Iteration 483
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 483: Reward 1.179 Loss 12.624 AE 0.025
Starting Iteration 484
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 484: Reward 1.144 Loss 11.684 AE 0.026
Starting Iteration 485
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 485: Reward 1.188 Loss 13.152 AE 0.025
Starting Iteration 486
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 486: Reward 1.166 Loss 13.053 AE 0.025
Starting Iteration 487
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 487: Reward 1.155 Loss 12.781 AE 0.024
Starting Iteration 488
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 488: Reward 1.178 Loss 12.517 AE 0.024
Starting Iteration 489
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 489: Reward 1.161 Loss 11.967 AE 0.024
Starting Iteration 490
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 490: Reward 1.162 Loss 12.763 AE 0.025
Starting Iteration 491
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 491: Reward 1.150 Loss 12.026 AE 0.025
Starting Iteration 492
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 492: Reward 1.155 Loss 12.254 AE 0.025
Starting Iteration 493
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 493: Reward 1.127 Loss 11.750 AE 0.026
Starting Iteration 494
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 494: Reward 1.161 Loss 11.906 AE 0.026
Starting Iteration 495
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 495: Reward 1.146 Loss 11.987 AE 0.027
Starting Iteration 496
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 496: Reward 1.164 Loss 12.890 AE 0.027
Starting Iteration 497
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 497: Reward 1.158 Loss 12.748 AE 0.029
Starting Iteration 498
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 498: Reward 1.119 Loss 11.344 AE 0.029
Starting Iteration 499
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 499: Reward 1.110 Loss 11.614 AE 0.029
Starting Iteration 500
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 500: Reward 1.127 Loss 11.330 AE 0.029
Saved checkpoint to policy_500.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 501
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 501: Reward 1.118 Loss 11.596 AE 0.029
Starting Iteration 502
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 502: Reward 1.120 Loss 12.044 AE 0.029
Starting Iteration 503
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 503: Reward 1.125 Loss 11.233 AE 0.029
Starting Iteration 504
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 504: Reward 1.150 Loss 12.311 AE 0.029
Starting Iteration 505
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 505: Reward 1.152 Loss 11.618 AE 0.028
Starting Iteration 506
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 506: Reward 1.143 Loss 11.807 AE 0.028
Starting Iteration 507
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 507: Reward 1.147 Loss 12.210 AE 0.028
Starting Iteration 508
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 508: Reward 1.142 Loss 11.707 AE 0.028
Starting Iteration 509
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 509: Reward 1.109 Loss 11.002 AE 0.028
Starting Iteration 510
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 510: Reward 1.146 Loss 12.184 AE 0.027
Starting Iteration 511
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 511: Reward 1.165 Loss 11.853 AE 0.027
Starting Iteration 512
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 512: Reward 1.169 Loss 12.134 AE 0.028
Starting Iteration 513
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 513: Reward 1.139 Loss 12.049 AE 0.027
Starting Iteration 514
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 514: Reward 1.151 Loss 12.642 AE 0.028
Starting Iteration 515
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 515: Reward 1.147 Loss 11.525 AE 0.025
Starting Iteration 516
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 516: Reward 1.154 Loss 12.646 AE 0.026
Starting Iteration 517
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 517: Reward 1.189 Loss 12.629 AE 0.026
Starting Iteration 518
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 518: Reward 1.158 Loss 11.799 AE 0.026
Starting Iteration 519
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 519: Reward 1.151 Loss 11.543 AE 0.026
Starting Iteration 520
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 520: Reward 1.166 Loss 11.368 AE 0.026
Starting Iteration 521
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 521: Reward 1.144 Loss 10.281 AE 0.026
Starting Iteration 522
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 522: Reward 1.163 Loss 11.801 AE 0.026
Starting Iteration 523
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 523: Reward 1.164 Loss 11.968 AE 0.026
Starting Iteration 524
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 524: Reward 1.128 Loss 10.849 AE 0.026
Starting Iteration 525
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 525: Reward 1.127 Loss 11.355 AE 0.027
Starting Iteration 526
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 526: Reward 1.133 Loss 11.014 AE 0.027
Starting Iteration 527
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 527: Reward 1.126 Loss 11.062 AE 0.027
Starting Iteration 528
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 528: Reward 1.143 Loss 11.155 AE 0.026
Starting Iteration 529
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 529: Reward 1.132 Loss 11.220 AE 0.026
Starting Iteration 530
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 530: Reward 1.134 Loss 11.717 AE 0.026
Starting Iteration 531
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 531: Reward 1.160 Loss 11.170 AE 0.026
Starting Iteration 532
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 532: Reward 1.154 Loss 11.045 AE 0.026
Starting Iteration 533
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 533: Reward 1.181 Loss 12.051 AE 0.026
Starting Iteration 534
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 534: Reward 1.141 Loss 10.850 AE 0.026
Starting Iteration 535
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 535: Reward 1.147 Loss 11.466 AE 0.024
Starting Iteration 536
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 536: Reward 1.167 Loss 11.966 AE 0.025
Starting Iteration 537
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 537: Reward 1.153 Loss 10.913 AE 0.025
Starting Iteration 538
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 538: Reward 1.139 Loss 10.998 AE 0.024
Starting Iteration 539
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 539: Reward 1.137 Loss 10.989 AE 0.025
Starting Iteration 540
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 540: Reward 1.168 Loss 11.510 AE 0.026
Starting Iteration 541
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 541: Reward 1.145 Loss 10.427 AE 0.025
Starting Iteration 542
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 542: Reward 1.156 Loss 11.345 AE 0.025
Starting Iteration 543
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 543: Reward 1.137 Loss 11.387 AE 0.024
Starting Iteration 544
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 544: Reward 1.189 Loss 12.146 AE 0.025
Starting Iteration 545
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 545: Reward 1.163 Loss 12.011 AE 0.024
Starting Iteration 546
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 546: Reward 1.178 Loss 11.859 AE 0.025
Starting Iteration 547
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 547: Reward 1.143 Loss 10.737 AE 0.025
Starting Iteration 548
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 548: Reward 1.133 Loss 11.014 AE 0.025
Starting Iteration 549
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 549: Reward 1.164 Loss 11.701 AE 0.025
Starting Iteration 550
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 550: Reward 1.120 Loss 10.612 AE 0.025
Saved checkpoint to policy_550.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 551
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 551: Reward 1.133 Loss 11.261 AE 0.026
Starting Iteration 552
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 552: Reward 1.126 Loss 11.022 AE 0.026
Starting Iteration 553
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 553: Reward 1.138 Loss 11.474 AE 0.026
Starting Iteration 554
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 554: Reward 1.122 Loss 10.952 AE 0.026
Starting Iteration 555
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 555: Reward 1.151 Loss 12.062 AE 0.027
Starting Iteration 556
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 556: Reward 1.123 Loss 10.672 AE 0.026
Starting Iteration 557
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 557: Reward 1.144 Loss 11.095 AE 0.027
Starting Iteration 558
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 558: Reward 1.124 Loss 10.370 AE 0.025
Starting Iteration 559
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 559: Reward 1.161 Loss 11.878 AE 0.025
Starting Iteration 560
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 560: Reward 1.131 Loss 10.831 AE 0.025
Starting Iteration 561
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 561: Reward 1.130 Loss 9.957 AE 0.025
Starting Iteration 562
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 562: Reward 1.189 Loss 11.371 AE 0.026
Starting Iteration 563
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 563: Reward 1.169 Loss 11.532 AE 0.025
Starting Iteration 564
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 564: Reward 1.165 Loss 11.200 AE 0.026
Starting Iteration 565
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 565: Reward 1.162 Loss 10.115 AE 0.027
Starting Iteration 566
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 566: Reward 1.153 Loss 10.652 AE 0.026
Starting Iteration 567
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 567: Reward 1.156 Loss 11.703 AE 0.026
Starting Iteration 568
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 568: Reward 1.132 Loss 10.909 AE 0.026
Starting Iteration 569
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 569: Reward 1.149 Loss 10.690 AE 0.026
Starting Iteration 570
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 570: Reward 1.168 Loss 10.816 AE 0.028
Starting Iteration 571
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 571: Reward 1.152 Loss 10.816 AE 0.028
Starting Iteration 572
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 572: Reward 1.149 Loss 10.557 AE 0.028
Starting Iteration 573
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 573: Reward 1.159 Loss 10.975 AE 0.027
Starting Iteration 574
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 574: Reward 1.150 Loss 10.728 AE 0.026
Starting Iteration 575
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 575: Reward 1.130 Loss 11.065 AE 0.026
Starting Iteration 576
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 576: Reward 1.135 Loss 11.370 AE 0.026
Starting Iteration 577
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 577: Reward 1.145 Loss 11.018 AE 0.027
Starting Iteration 578
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 578: Reward 1.164 Loss 11.988 AE 0.027
Starting Iteration 579
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 579: Reward 1.143 Loss 11.015 AE 0.028
Starting Iteration 580
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 580: Reward 1.131 Loss 10.089 AE 0.027
Starting Iteration 581
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 581: Reward 1.162 Loss 12.208 AE 0.027
Starting Iteration 582
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 582: Reward 1.110 Loss 10.713 AE 0.028
Starting Iteration 583
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 583: Reward 1.128 Loss 10.982 AE 0.028
Starting Iteration 584
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 584: Reward 1.142 Loss 11.320 AE 0.027
Starting Iteration 585
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 585: Reward 1.156 Loss 11.140 AE 0.026
Starting Iteration 586
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 586: Reward 1.140 Loss 10.832 AE 0.026
Starting Iteration 587
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 587: Reward 1.163 Loss 10.962 AE 0.026
Starting Iteration 588
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 588: Reward 1.157 Loss 11.017 AE 0.026
Starting Iteration 589
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 589: Reward 1.161 Loss 11.264 AE 0.026
Starting Iteration 590
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 590: Reward 1.180 Loss 11.492 AE 0.026
Starting Iteration 591
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 591: Reward 1.159 Loss 10.340 AE 0.027
Starting Iteration 592
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 592: Reward 1.152 Loss 10.295 AE 0.026
Starting Iteration 593
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 593: Reward 1.129 Loss 9.992 AE 0.025
Starting Iteration 594
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 594: Reward 1.178 Loss 11.324 AE 0.025
Starting Iteration 595
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 595: Reward 1.152 Loss 10.076 AE 0.026
Starting Iteration 596
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 596: Reward 1.160 Loss 10.982 AE 0.026
Starting Iteration 597
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 597: Reward 1.159 Loss 10.615 AE 0.027
Starting Iteration 598
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 598: Reward 1.144 Loss 10.216 AE 0.027
Starting Iteration 599
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 599: Reward 1.150 Loss 10.648 AE 0.027
Starting Iteration 600
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 600: Reward 1.167 Loss 10.915 AE 0.026
Saved checkpoint to policy_600.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 601
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 601: Reward 1.138 Loss 10.252 AE 0.025
Starting Iteration 602
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 602: Reward 1.154 Loss 10.592 AE 0.026
Starting Iteration 603
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 603: Reward 1.153 Loss 9.655 AE 0.025
Starting Iteration 604
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 604: Reward 1.154 Loss 12.091 AE 0.025
Starting Iteration 605
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 605: Reward 1.170 Loss 11.251 AE 0.024
Starting Iteration 606
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 606: Reward 1.163 Loss 11.411 AE 0.025
Starting Iteration 607
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 607: Reward 1.144 Loss 11.043 AE 0.023
Starting Iteration 608
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 608: Reward 1.150 Loss 11.543 AE 0.023
Starting Iteration 609
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 609: Reward 1.129 Loss 11.235 AE 0.022
Starting Iteration 610
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 610: Reward 1.128 Loss 12.075 AE 0.022
Starting Iteration 611
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 611: Reward 1.145 Loss 12.674 AE 0.021
Starting Iteration 612
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 612: Reward 1.134 Loss 12.733 AE 0.021
Starting Iteration 613
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 613: Reward 1.095 Loss 11.543 AE 0.021
Starting Iteration 614
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 614: Reward 1.100 Loss 12.306 AE 0.020
Starting Iteration 615
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 615: Reward 1.112 Loss 12.487 AE 0.020
Starting Iteration 616
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 616: Reward 1.138 Loss 13.288 AE 0.021
Starting Iteration 617
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 617: Reward 1.115 Loss 12.369 AE 0.020
Starting Iteration 618
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 618: Reward 1.150 Loss 12.816 AE 0.020
Starting Iteration 619
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 619: Reward 1.158 Loss 11.855 AE 0.022
Starting Iteration 620
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 620: Reward 1.149 Loss 11.827 AE 0.023
Starting Iteration 621
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 621: Reward 1.124 Loss 11.942 AE 0.023
Starting Iteration 622
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 622: Reward 1.151 Loss 11.857 AE 0.024
Starting Iteration 623
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 623: Reward 1.127 Loss 11.528 AE 0.024
Starting Iteration 624
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 624: Reward 1.147 Loss 11.795 AE 0.023
Starting Iteration 625
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 625: Reward 1.149 Loss 11.913 AE 0.023
Starting Iteration 626
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 626: Reward 1.168 Loss 11.601 AE 0.024
Starting Iteration 627
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 627: Reward 1.154 Loss 11.466 AE 0.023
Starting Iteration 628
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 628: Reward 1.144 Loss 11.049 AE 0.025
Starting Iteration 629
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 629: Reward 1.143 Loss 11.238 AE 0.026
Starting Iteration 630
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 630: Reward 1.137 Loss 11.039 AE 0.026
Starting Iteration 631
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 631: Reward 1.165 Loss 11.104 AE 0.026
Starting Iteration 632
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 632: Reward 1.134 Loss 10.604 AE 0.025
Starting Iteration 633
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 633: Reward 1.156 Loss 11.175 AE 0.025
Starting Iteration 634
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 634: Reward 1.163 Loss 10.716 AE 0.026
Starting Iteration 635
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 635: Reward 1.171 Loss 11.448 AE 0.027
Starting Iteration 636
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 636: Reward 1.166 Loss 11.056 AE 0.027
Starting Iteration 637
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 637: Reward 1.132 Loss 10.027 AE 0.027
Starting Iteration 638
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 638: Reward 1.154 Loss 10.305 AE 0.027
Starting Iteration 639
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 639: Reward 1.144 Loss 10.700 AE 0.027
Starting Iteration 640
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 640: Reward 1.147 Loss 10.566 AE 0.026
Starting Iteration 641
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 641: Reward 1.147 Loss 11.702 AE 0.025
Starting Iteration 642
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 642: Reward 1.154 Loss 10.865 AE 0.025
Starting Iteration 643
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 643: Reward 1.136 Loss 9.398 AE 0.025
Starting Iteration 644
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 644: Reward 1.122 Loss 10.703 AE 0.026
Starting Iteration 645
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 645: Reward 1.147 Loss 10.765 AE 0.026
Starting Iteration 646
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 646: Reward 1.134 Loss 9.811 AE 0.025
Starting Iteration 647
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 647: Reward 1.162 Loss 11.341 AE 0.025
Starting Iteration 648
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 648: Reward 1.152 Loss 11.674 AE 0.025
Starting Iteration 649
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 649: Reward 1.125 Loss 9.815 AE 0.024
Starting Iteration 650
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 650: Reward 1.148 Loss 11.646 AE 0.025
Saved checkpoint to policy_650.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 651
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 651: Reward 1.182 Loss 11.006 AE 0.025
Starting Iteration 652
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 652: Reward 1.157 Loss 11.105 AE 0.025
Starting Iteration 653
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 653: Reward 1.157 Loss 10.053 AE 0.026
Starting Iteration 654
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 654: Reward 1.167 Loss 10.570 AE 0.026
Starting Iteration 655
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 655: Reward 1.147 Loss 10.381 AE 0.025
Starting Iteration 656
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 656: Reward 1.146 Loss 10.126 AE 0.027
Starting Iteration 657
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 657: Reward 1.150 Loss 11.888 AE 0.026
Starting Iteration 658
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 658: Reward 1.176 Loss 11.917 AE 0.025
Starting Iteration 659
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 659: Reward 1.111 Loss 10.291 AE 0.026
Starting Iteration 660
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 660: Reward 1.173 Loss 11.631 AE 0.026
Starting Iteration 661
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 661: Reward 1.128 Loss 10.077 AE 0.026
Starting Iteration 662
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 662: Reward 1.167 Loss 10.730 AE 0.027
Starting Iteration 663
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 663: Reward 1.129 Loss 9.439 AE 0.027
Starting Iteration 664
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 664: Reward 1.158 Loss 10.693 AE 0.026
Starting Iteration 665
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 665: Reward 1.137 Loss 10.523 AE 0.026
Starting Iteration 666
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 666: Reward 1.133 Loss 10.470 AE 0.026
Starting Iteration 667
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 667: Reward 1.156 Loss 10.323 AE 0.025
Starting Iteration 668
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 668: Reward 1.153 Loss 10.629 AE 0.025
Starting Iteration 669
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 669: Reward 1.141 Loss 10.197 AE 0.026
Starting Iteration 670
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 670: Reward 1.136 Loss 10.750 AE 0.026
Starting Iteration 671
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 671: Reward 1.155 Loss 10.630 AE 0.026
Starting Iteration 672
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 672: Reward 1.170 Loss 10.564 AE 0.026
Starting Iteration 673
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 673: Reward 1.168 Loss 10.334 AE 0.026
Starting Iteration 674
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 674: Reward 1.154 Loss 10.143 AE 0.026
Starting Iteration 675
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 675: Reward 1.146 Loss 10.453 AE 0.026
Starting Iteration 676
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 676: Reward 1.163 Loss 10.129 AE 0.026
Starting Iteration 677
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 677: Reward 1.145 Loss 10.368 AE 0.027
Starting Iteration 678
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 678: Reward 1.157 Loss 10.822 AE 0.026
Starting Iteration 679
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 679: Reward 1.146 Loss 10.717 AE 0.027
Starting Iteration 680
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 680: Reward 1.147 Loss 10.884 AE 0.026
Starting Iteration 681
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 681: Reward 1.176 Loss 11.399 AE 0.026
Starting Iteration 682
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 682: Reward 1.153 Loss 10.533 AE 0.026
Starting Iteration 683
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 683: Reward 1.163 Loss 10.711 AE 0.027
Starting Iteration 684
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 684: Reward 1.158 Loss 10.977 AE 0.028
Starting Iteration 685
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 685: Reward 1.144 Loss 10.111 AE 0.028
Starting Iteration 686
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 686: Reward 1.139 Loss 10.116 AE 0.026
Starting Iteration 687
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 687: Reward 1.155 Loss 10.040 AE 0.027
Starting Iteration 688
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 688: Reward 1.148 Loss 10.631 AE 0.026
Starting Iteration 689
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 689: Reward 1.167 Loss 10.542 AE 0.027
Starting Iteration 690
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 690: Reward 1.166 Loss 10.167 AE 0.028
Starting Iteration 691
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 691: Reward 1.153 Loss 9.917 AE 0.027
Starting Iteration 692
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 692: Reward 1.155 Loss 10.500 AE 0.028
Starting Iteration 693
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 693: Reward 1.148 Loss 10.438 AE 0.027
Starting Iteration 694
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 694: Reward 1.140 Loss 10.585 AE 0.027
Starting Iteration 695
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 695: Reward 1.157 Loss 10.566 AE 0.028
Starting Iteration 696
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 696: Reward 1.160 Loss 10.706 AE 0.026
Starting Iteration 697
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 697: Reward 1.171 Loss 10.313 AE 0.027
Starting Iteration 698
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 698: Reward 1.146 Loss 10.292 AE 0.027
Starting Iteration 699
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 699: Reward 1.142 Loss 10.446 AE 0.026
Starting Iteration 700
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 700: Reward 1.130 Loss 9.437 AE 0.027
Saved checkpoint to policy_700.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 701
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 701: Reward 1.156 Loss 10.799 AE 0.026
Starting Iteration 702
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 702: Reward 1.133 Loss 9.938 AE 0.026
Starting Iteration 703
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 703: Reward 1.166 Loss 10.352 AE 0.026
Starting Iteration 704
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 704: Reward 1.140 Loss 10.552 AE 0.026
Starting Iteration 705
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 705: Reward 1.167 Loss 10.669 AE 0.026
Starting Iteration 706
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 706: Reward 1.168 Loss 11.014 AE 0.026
Starting Iteration 707
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 707: Reward 1.149 Loss 10.546 AE 0.026
Starting Iteration 708
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 708: Reward 1.146 Loss 10.855 AE 0.026
Starting Iteration 709
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 709: Reward 1.172 Loss 11.676 AE 0.026
Starting Iteration 710
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 710: Reward 1.152 Loss 11.167 AE 0.026
Starting Iteration 711
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 711: Reward 1.147 Loss 10.899 AE 0.026
Starting Iteration 712
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 712: Reward 1.161 Loss 11.790 AE 0.027
Starting Iteration 713
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 713: Reward 1.166 Loss 10.930 AE 0.027
Starting Iteration 714
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 714: Reward 1.140 Loss 11.388 AE 0.027
Starting Iteration 715
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 715: Reward 1.117 Loss 10.429 AE 0.027
Starting Iteration 716
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 716: Reward 1.140 Loss 9.617 AE 0.028
Starting Iteration 717
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 717: Reward 1.119 Loss 9.174 AE 0.027
Starting Iteration 718
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 718: Reward 1.157 Loss 11.748 AE 0.027
Starting Iteration 719
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 719: Reward 1.146 Loss 11.476 AE 0.027
Starting Iteration 720
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 720: Reward 1.144 Loss 11.027 AE 0.027
Starting Iteration 721
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 721: Reward 1.175 Loss 10.468 AE 0.027
Starting Iteration 722
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 722: Reward 1.125 Loss 9.752 AE 0.027
Starting Iteration 723
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 723: Reward 1.165 Loss 9.973 AE 0.027
Starting Iteration 724
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 724: Reward 1.177 Loss 10.708 AE 0.027
Starting Iteration 725
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 725: Reward 1.147 Loss 10.442 AE 0.026
Starting Iteration 726
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 726: Reward 1.163 Loss 11.282 AE 0.027
Starting Iteration 727
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 727: Reward 1.157 Loss 10.467 AE 0.026
Starting Iteration 728
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 728: Reward 1.132 Loss 9.825 AE 0.026
Starting Iteration 729
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 729: Reward 1.163 Loss 10.365 AE 0.027
Starting Iteration 730
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 730: Reward 1.146 Loss 9.783 AE 0.027
Starting Iteration 731
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 731: Reward 1.160 Loss 11.275 AE 0.027
Starting Iteration 732
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 732: Reward 1.143 Loss 9.722 AE 0.027
Starting Iteration 733
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 733: Reward 1.185 Loss 10.530 AE 0.027
Starting Iteration 734
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 734: Reward 1.151 Loss 10.158 AE 0.025
Starting Iteration 735
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 735: Reward 1.144 Loss 9.944 AE 0.025
Starting Iteration 736
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 736: Reward 1.161 Loss 10.092 AE 0.025
Starting Iteration 737
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 737: Reward 1.120 Loss 9.823 AE 0.025
Starting Iteration 738
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 738: Reward 1.182 Loss 11.498 AE 0.026
Starting Iteration 739
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 739: Reward 1.189 Loss 11.998 AE 0.025
Starting Iteration 740
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 740: Reward 1.171 Loss 10.936 AE 0.026
Starting Iteration 741
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 741: Reward 1.153 Loss 10.584 AE 0.026
Starting Iteration 742
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 742: Reward 1.151 Loss 9.779 AE 0.026
Starting Iteration 743
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 743: Reward 1.145 Loss 10.364 AE 0.026
Starting Iteration 744
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 744: Reward 1.133 Loss 10.519 AE 0.026
Starting Iteration 745
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 745: Reward 1.169 Loss 11.460 AE 0.026
Starting Iteration 746
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 746: Reward 1.137 Loss 10.304 AE 0.026
Starting Iteration 747
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 747: Reward 1.128 Loss 10.107 AE 0.026
Starting Iteration 748
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 748: Reward 1.150 Loss 10.543 AE 0.026
Starting Iteration 749
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 749: Reward 1.148 Loss 10.135 AE 0.027
Starting Iteration 750
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 750: Reward 1.169 Loss 10.847 AE 0.027
Saved checkpoint to policy_750.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 751
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 751: Reward 1.160 Loss 9.893 AE 0.027
Starting Iteration 752
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 752: Reward 1.158 Loss 10.532 AE 0.027
Starting Iteration 753
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 753: Reward 1.154 Loss 10.473 AE 0.026
Starting Iteration 754
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 754: Reward 1.148 Loss 10.879 AE 0.027
Starting Iteration 755
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 755: Reward 1.160 Loss 10.617 AE 0.027
Starting Iteration 756
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 756: Reward 1.185 Loss 11.195 AE 0.027
Starting Iteration 757
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 757: Reward 1.143 Loss 10.086 AE 0.026
Starting Iteration 758
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 758: Reward 1.161 Loss 10.277 AE 0.026
Starting Iteration 759
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 759: Reward 1.160 Loss 10.204 AE 0.027
Starting Iteration 760
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 760: Reward 1.157 Loss 10.400 AE 0.026
Starting Iteration 761
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 761: Reward 1.137 Loss 9.772 AE 0.025
Starting Iteration 762
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 762: Reward 1.198 Loss 11.710 AE 0.025
Starting Iteration 763
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 763: Reward 1.157 Loss 10.597 AE 0.026
Starting Iteration 764
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 764: Reward 1.148 Loss 9.966 AE 0.025
Starting Iteration 765
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 765: Reward 1.179 Loss 11.153 AE 0.025
Starting Iteration 766
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 766: Reward 1.150 Loss 10.574 AE 0.024
Starting Iteration 767
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 767: Reward 1.199 Loss 12.021 AE 0.025
Starting Iteration 768
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 768: Reward 1.178 Loss 11.037 AE 0.023
Starting Iteration 769
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 769: Reward 1.154 Loss 11.073 AE 0.024
Starting Iteration 770
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 770: Reward 1.147 Loss 10.698 AE 0.024
Starting Iteration 771
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 771: Reward 1.128 Loss 10.222 AE 0.024
Starting Iteration 772
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 772: Reward 1.163 Loss 10.525 AE 0.023
Starting Iteration 773
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 773: Reward 1.180 Loss 11.416 AE 0.024
Starting Iteration 774
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 774: Reward 1.153 Loss 10.711 AE 0.024
Starting Iteration 775
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 775: Reward 1.138 Loss 11.371 AE 0.024
Starting Iteration 776
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 776: Reward 1.174 Loss 10.957 AE 0.023
Starting Iteration 777
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 777: Reward 1.173 Loss 11.084 AE 0.024
Starting Iteration 778
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 778: Reward 1.154 Loss 10.378 AE 0.024
Starting Iteration 779
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 779: Reward 1.167 Loss 10.114 AE 0.024
Starting Iteration 780
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 780: Reward 1.145 Loss 9.634 AE 0.024
Starting Iteration 781
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 781: Reward 1.159 Loss 11.062 AE 0.024
Starting Iteration 782
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 782: Reward 1.151 Loss 10.597 AE 0.024
Starting Iteration 783
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 783: Reward 1.142 Loss 10.221 AE 0.023
Starting Iteration 784
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 784: Reward 1.157 Loss 10.249 AE 0.024
Starting Iteration 785
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 785: Reward 1.183 Loss 10.620 AE 0.024
Starting Iteration 786
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 786: Reward 1.146 Loss 10.448 AE 0.024
Starting Iteration 787
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 787: Reward 1.139 Loss 10.566 AE 0.024
Starting Iteration 788
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 788: Reward 1.177 Loss 10.940 AE 0.025
Starting Iteration 789
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 789: Reward 1.173 Loss 10.290 AE 0.024
Starting Iteration 790
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 790: Reward 1.164 Loss 11.479 AE 0.024
Starting Iteration 791
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 791: Reward 1.158 Loss 10.360 AE 0.025
Starting Iteration 792
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 792: Reward 1.145 Loss 9.550 AE 0.025
Starting Iteration 793
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 793: Reward 1.186 Loss 10.578 AE 0.025
Starting Iteration 794
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 794: Reward 1.124 Loss 9.171 AE 0.024
Starting Iteration 795
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 795: Reward 1.159 Loss 10.694 AE 0.024
Starting Iteration 796
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 796: Reward 1.175 Loss 12.044 AE 0.025
Starting Iteration 797
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 797: Reward 1.137 Loss 10.416 AE 0.024
Starting Iteration 798
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 798: Reward 1.142 Loss 10.308 AE 0.025
Starting Iteration 799
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 799: Reward 1.156 Loss 11.150 AE 0.024
Starting Iteration 800
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 800: Reward 1.124 Loss 9.211 AE 0.024
Saved checkpoint to policy_800.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 801
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 801: Reward 1.163 Loss 11.118 AE 0.025
Starting Iteration 802
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 802: Reward 1.175 Loss 9.927 AE 0.026
Starting Iteration 803
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 803: Reward 1.150 Loss 9.593 AE 0.026
Starting Iteration 804
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 804: Reward 1.143 Loss 10.161 AE 0.026
Starting Iteration 805
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 805: Reward 1.147 Loss 9.727 AE 0.026
Starting Iteration 806
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 806: Reward 1.162 Loss 9.533 AE 0.028
Starting Iteration 807
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 807: Reward 1.150 Loss 10.010 AE 0.027
Starting Iteration 808
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 808: Reward 1.156 Loss 10.245 AE 0.028
Starting Iteration 809
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 809: Reward 1.176 Loss 10.318 AE 0.028
Starting Iteration 810
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 810: Reward 1.147 Loss 9.403 AE 0.028
Starting Iteration 811
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 811: Reward 1.140 Loss 9.886 AE 0.028
Starting Iteration 812
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 812: Reward 1.117 Loss 9.012 AE 0.029
Starting Iteration 813
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 813: Reward 1.150 Loss 9.721 AE 0.028
Starting Iteration 814
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 814: Reward 1.109 Loss 9.245 AE 0.028
Starting Iteration 815
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 815: Reward 1.146 Loss 9.121 AE 0.027
Starting Iteration 816
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 816: Reward 1.131 Loss 9.624 AE 0.026
Starting Iteration 817
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 817: Reward 1.155 Loss 9.547 AE 0.027
Starting Iteration 818
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 818: Reward 1.134 Loss 9.843 AE 0.027
Starting Iteration 819
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 819: Reward 1.147 Loss 9.655 AE 0.027
Starting Iteration 820
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 820: Reward 1.163 Loss 9.993 AE 0.027
Starting Iteration 821
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 821: Reward 1.140 Loss 9.138 AE 0.026
Starting Iteration 822
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 822: Reward 1.150 Loss 9.229 AE 0.026
Starting Iteration 823
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 823: Reward 1.161 Loss 10.398 AE 0.026
Starting Iteration 824
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 824: Reward 1.179 Loss 11.419 AE 0.025
Starting Iteration 825
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 825: Reward 1.158 Loss 11.275 AE 0.026
Starting Iteration 826
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 826: Reward 1.186 Loss 10.394 AE 0.026
Starting Iteration 827
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 827: Reward 1.203 Loss 11.628 AE 0.026
Starting Iteration 828
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 828: Reward 1.159 Loss 11.187 AE 0.026
Starting Iteration 829
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 829: Reward 1.165 Loss 11.070 AE 0.026
Starting Iteration 830
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 830: Reward 1.114 Loss 10.218 AE 0.027
Starting Iteration 831
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 831: Reward 1.129 Loss 10.372 AE 0.028
Starting Iteration 832
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 832: Reward 1.169 Loss 12.223 AE 0.028
Starting Iteration 833
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 833: Reward 1.157 Loss 12.084 AE 0.028
Starting Iteration 834
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 834: Reward 1.136 Loss 10.707 AE 0.028
Starting Iteration 835
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 835: Reward 1.147 Loss 10.871 AE 0.028
Starting Iteration 836
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 836: Reward 1.120 Loss 9.955 AE 0.028
Starting Iteration 837
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 837: Reward 1.196 Loss 12.506 AE 0.028
Starting Iteration 838
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 838: Reward 1.129 Loss 10.305 AE 0.029
Starting Iteration 839
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 839: Reward 1.129 Loss 9.930 AE 0.029
Starting Iteration 840
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 840: Reward 1.130 Loss 10.987 AE 0.029
Starting Iteration 841
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 841: Reward 1.162 Loss 11.693 AE 0.028
Starting Iteration 842
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 842: Reward 1.110 Loss 9.415 AE 0.028
Starting Iteration 843
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 843: Reward 1.153 Loss 12.396 AE 0.027
Starting Iteration 844
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 844: Reward 1.127 Loss 9.996 AE 0.028
Starting Iteration 845
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 845: Reward 1.138 Loss 10.504 AE 0.028
Starting Iteration 846
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 846: Reward 1.124 Loss 9.796 AE 0.027
Starting Iteration 847
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 847: Reward 1.138 Loss 9.241 AE 0.028
Starting Iteration 848
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 848: Reward 1.169 Loss 11.112 AE 0.026
Starting Iteration 849
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 849: Reward 1.148 Loss 10.875 AE 0.026
Starting Iteration 850
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 850: Reward 1.147 Loss 10.670 AE 0.027
Saved checkpoint to policy_850.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 851
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 851: Reward 1.134 Loss 9.653 AE 0.027
Starting Iteration 852
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 852: Reward 1.133 Loss 9.852 AE 0.028
Starting Iteration 853
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 853: Reward 1.157 Loss 10.403 AE 0.027
Starting Iteration 854
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 854: Reward 1.153 Loss 10.696 AE 0.027
Starting Iteration 855
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 855: Reward 1.165 Loss 9.932 AE 0.027
Starting Iteration 856
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 856: Reward 1.172 Loss 10.400 AE 0.027
Starting Iteration 857
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 857: Reward 1.177 Loss 9.615 AE 0.028
Starting Iteration 858
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 858: Reward 1.146 Loss 9.216 AE 0.026
Starting Iteration 859
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 859: Reward 1.143 Loss 9.459 AE 0.027
Starting Iteration 860
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 860: Reward 1.136 Loss 9.638 AE 0.027
Starting Iteration 861
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 861: Reward 1.145 Loss 9.662 AE 0.027
Starting Iteration 862
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 862: Reward 1.169 Loss 9.934 AE 0.028
Starting Iteration 863
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 863: Reward 1.145 Loss 9.470 AE 0.027
Starting Iteration 864
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 864: Reward 1.171 Loss 9.600 AE 0.029
Starting Iteration 865
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 865: Reward 1.135 Loss 9.805 AE 0.028
Starting Iteration 866
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 866: Reward 1.145 Loss 10.059 AE 0.027
Starting Iteration 867
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 867: Reward 1.188 Loss 10.989 AE 0.028
Starting Iteration 868
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 868: Reward 1.161 Loss 8.985 AE 0.028
Starting Iteration 869
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 869: Reward 1.157 Loss 9.743 AE 0.028
Starting Iteration 870
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 870: Reward 1.155 Loss 10.247 AE 0.028
Starting Iteration 871
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 871: Reward 1.186 Loss 9.329 AE 0.028
Starting Iteration 872
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 872: Reward 1.161 Loss 9.782 AE 0.028
Starting Iteration 873
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 873: Reward 1.168 Loss 9.863 AE 0.028
Starting Iteration 874
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 874: Reward 1.141 Loss 8.971 AE 0.028
Starting Iteration 875
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 875: Reward 1.168 Loss 9.576 AE 0.026
Starting Iteration 876
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 876: Reward 1.157 Loss 10.256 AE 0.027
Starting Iteration 877
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 877: Reward 1.178 Loss 9.457 AE 0.026
Starting Iteration 878
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 878: Reward 1.143 Loss 10.007 AE 0.026
Starting Iteration 879
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 879: Reward 1.143 Loss 9.705 AE 0.025
Starting Iteration 880
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 880: Reward 1.161 Loss 10.231 AE 0.025
Starting Iteration 881
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 881: Reward 1.155 Loss 10.270 AE 0.025
Starting Iteration 882
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 882: Reward 1.129 Loss 9.305 AE 0.024
Starting Iteration 883
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 883: Reward 1.128 Loss 10.376 AE 0.024
Starting Iteration 884
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 884: Reward 1.127 Loss 10.437 AE 0.023
Starting Iteration 885
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 885: Reward 1.144 Loss 10.529 AE 0.023
Starting Iteration 886
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 886: Reward 1.156 Loss 10.904 AE 0.024
Starting Iteration 887
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 887: Reward 1.146 Loss 10.452 AE 0.024
Starting Iteration 888
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 888: Reward 1.167 Loss 10.171 AE 0.024
Starting Iteration 889
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 889: Reward 1.132 Loss 10.358 AE 0.023
Starting Iteration 890
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 890: Reward 1.117 Loss 12.110 AE 0.023
Starting Iteration 891
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 891: Reward 1.151 Loss 11.241 AE 0.023
Starting Iteration 892
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 892: Reward 1.162 Loss 11.371 AE 0.023
Starting Iteration 893
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 893: Reward 1.180 Loss 11.406 AE 0.023
Starting Iteration 894
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 894: Reward 1.168 Loss 11.495 AE 0.024
Starting Iteration 895
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 895: Reward 1.161 Loss 10.082 AE 0.023
Starting Iteration 896
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 896: Reward 1.155 Loss 9.765 AE 0.024
Starting Iteration 897
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 897: Reward 1.172 Loss 11.176 AE 0.024
Starting Iteration 898
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 898: Reward 1.137 Loss 10.329 AE 0.024
Starting Iteration 899
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 899: Reward 1.182 Loss 10.781 AE 0.024
Starting Iteration 900
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 900: Reward 1.140 Loss 9.240 AE 0.025
Saved checkpoint to policy_900.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 901
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 901: Reward 1.159 Loss 10.534 AE 0.025
Starting Iteration 902
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 902: Reward 1.167 Loss 9.989 AE 0.026
Starting Iteration 903
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 903: Reward 1.153 Loss 9.899 AE 0.025
Starting Iteration 904
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 904: Reward 1.163 Loss 10.210 AE 0.026
Starting Iteration 905
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 905: Reward 1.197 Loss 10.787 AE 0.027
Starting Iteration 906
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 906: Reward 1.180 Loss 10.353 AE 0.027
Starting Iteration 907
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 907: Reward 1.148 Loss 10.063 AE 0.027
Starting Iteration 908
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 908: Reward 1.187 Loss 9.964 AE 0.027
Starting Iteration 909
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 909: Reward 1.135 Loss 9.400 AE 0.027
Starting Iteration 910
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 910: Reward 1.127 Loss 9.496 AE 0.027
Starting Iteration 911
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 911: Reward 1.180 Loss 9.608 AE 0.027
Starting Iteration 912
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 912: Reward 1.164 Loss 9.854 AE 0.028
Starting Iteration 913
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 913: Reward 1.166 Loss 9.052 AE 0.027
Starting Iteration 914
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 914: Reward 1.161 Loss 9.793 AE 0.027
Starting Iteration 915
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 915: Reward 1.149 Loss 8.949 AE 0.026
Starting Iteration 916
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 916: Reward 1.143 Loss 10.315 AE 0.027
Starting Iteration 917
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 917: Reward 1.185 Loss 10.651 AE 0.027
Starting Iteration 918
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 918: Reward 1.139 Loss 9.901 AE 0.027
Starting Iteration 919
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 919: Reward 1.153 Loss 9.764 AE 0.027
Starting Iteration 920
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 920: Reward 1.147 Loss 10.337 AE 0.026
Starting Iteration 921
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 921: Reward 1.138 Loss 9.411 AE 0.028
Starting Iteration 922
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 922: Reward 1.108 Loss 8.859 AE 0.027
Starting Iteration 923
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 923: Reward 1.158 Loss 10.117 AE 0.026
Starting Iteration 924
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 924: Reward 1.149 Loss 9.726 AE 0.027
Starting Iteration 925
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 925: Reward 1.134 Loss 8.646 AE 0.027
Starting Iteration 926
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 926: Reward 1.183 Loss 9.990 AE 0.028
Starting Iteration 927
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 927: Reward 1.139 Loss 9.635 AE 0.028
Starting Iteration 928
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 928: Reward 1.152 Loss 9.655 AE 0.028
Starting Iteration 929
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 929: Reward 1.137 Loss 8.779 AE 0.028
Starting Iteration 930
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 930: Reward 1.155 Loss 9.193 AE 0.027
Starting Iteration 931
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 931: Reward 1.163 Loss 10.012 AE 0.028
Starting Iteration 932
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 932: Reward 1.143 Loss 9.225 AE 0.028
Starting Iteration 933
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 933: Reward 1.137 Loss 8.884 AE 0.029
Starting Iteration 934
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 934: Reward 1.154 Loss 9.666 AE 0.029
Starting Iteration 935
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 935: Reward 1.129 Loss 8.720 AE 0.029
Starting Iteration 936
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 936: Reward 1.114 Loss 9.146 AE 0.028
Starting Iteration 937
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 937: Reward 1.118 Loss 8.160 AE 0.029
Starting Iteration 938
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 938: Reward 1.161 Loss 10.271 AE 0.028
Starting Iteration 939
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 939: Reward 1.166 Loss 9.620 AE 0.028
Starting Iteration 940
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 940: Reward 1.136 Loss 9.302 AE 0.029
Starting Iteration 941
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 941: Reward 1.137 Loss 9.184 AE 0.030
Starting Iteration 942
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 942: Reward 1.095 Loss 8.446 AE 0.029
Starting Iteration 943
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 943: Reward 1.173 Loss 9.971 AE 0.029
Starting Iteration 944
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 944: Reward 1.107 Loss 8.240 AE 0.029
Starting Iteration 945
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 945: Reward 1.140 Loss 8.984 AE 0.028
Starting Iteration 946
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 946: Reward 1.145 Loss 8.818 AE 0.028
Starting Iteration 947
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 947: Reward 1.151 Loss 9.372 AE 0.029
Starting Iteration 948
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 948: Reward 1.156 Loss 9.607 AE 0.028
Starting Iteration 949
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 949: Reward 1.149 Loss 9.035 AE 0.027
Starting Iteration 950
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 950: Reward 1.173 Loss 9.963 AE 0.027
Saved checkpoint to policy_950.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
Starting Iteration 951
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 951: Reward 1.167 Loss 9.295 AE 0.028
Starting Iteration 952
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 952: Reward 1.164 Loss 9.708 AE 0.028
Starting Iteration 953
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 953: Reward 1.152 Loss 9.323 AE 0.028
Starting Iteration 954
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 954: Reward 1.163 Loss 10.089 AE 0.028
Starting Iteration 955
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 955: Reward 1.173 Loss 10.705 AE 0.027
Starting Iteration 956
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 956: Reward 1.160 Loss 10.663 AE 0.027
Starting Iteration 957
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 957: Reward 1.116 Loss 9.682 AE 0.028
Starting Iteration 958
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 958: Reward 1.129 Loss 11.203 AE 0.027
Starting Iteration 959
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 959: Reward 1.120 Loss 10.731 AE 0.027
Starting Iteration 960
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 960: Reward 1.174 Loss 12.985 AE 0.025
Starting Iteration 961
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 961: Reward 1.141 Loss 11.985 AE 0.025
Starting Iteration 962
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 962: Reward 1.126 Loss 11.754 AE 0.025
Starting Iteration 963
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 963: Reward 1.130 Loss 12.195 AE 0.024
Starting Iteration 964
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 964: Reward 1.114 Loss 11.068 AE 0.024
Starting Iteration 965
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 965: Reward 1.173 Loss 11.822 AE 0.024
Starting Iteration 966
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 966: Reward 1.134 Loss 10.183 AE 0.023
Starting Iteration 967
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 967: Reward 1.110 Loss 11.019 AE 0.024
Starting Iteration 968
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 968: Reward 1.106 Loss 10.320 AE 0.023
Starting Iteration 969
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 969: Reward 1.134 Loss 12.427 AE 0.023
Starting Iteration 970
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 970: Reward 1.135 Loss 11.414 AE 0.022
Starting Iteration 971
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 971: Reward 1.124 Loss 10.601 AE 0.022
Starting Iteration 972
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 972: Reward 1.144 Loss 10.736 AE 0.022
Starting Iteration 973
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 973: Reward 1.152 Loss 12.054 AE 0.021
Starting Iteration 974
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 974: Reward 1.132 Loss 10.997 AE 0.022
Starting Iteration 975
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 975: Reward 1.166 Loss 10.959 AE 0.023
Starting Iteration 976
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 976: Reward 1.147 Loss 11.590 AE 0.022
Starting Iteration 977
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 977: Reward 1.147 Loss 10.520 AE 0.022
Starting Iteration 978
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 978: Reward 1.127 Loss 10.128 AE 0.022
Starting Iteration 979
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 979: Reward 1.134 Loss 10.363 AE 0.022
Starting Iteration 980
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 980: Reward 1.142 Loss 11.127 AE 0.023
Starting Iteration 981
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 981: Reward 1.148 Loss 10.629 AE 0.023
Starting Iteration 982
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 982: Reward 1.170 Loss 10.955 AE 0.023
Starting Iteration 983
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 983: Reward 1.132 Loss 10.985 AE 0.024
Starting Iteration 984
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 984: Reward 1.143 Loss 10.389 AE 0.024
Starting Iteration 985
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 985: Reward 1.167 Loss 9.828 AE 0.025
Starting Iteration 986
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 986: Reward 1.140 Loss 10.194 AE 0.024
Starting Iteration 987
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 987: Reward 1.140 Loss 9.625 AE 0.024
Starting Iteration 988
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 988: Reward 1.159 Loss 10.273 AE 0.024
Starting Iteration 989
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 989: Reward 1.160 Loss 9.395 AE 0.026
Starting Iteration 990
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 990: Reward 1.186 Loss 10.876 AE 0.026
Starting Iteration 991
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 991: Reward 1.175 Loss 9.994 AE 0.026
Starting Iteration 992
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 992: Reward 1.160 Loss 9.890 AE 0.026
Starting Iteration 993
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 993: Reward 1.174 Loss 9.761 AE 0.026
Starting Iteration 994
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 994: Reward 1.151 Loss 9.901 AE 0.026
Starting Iteration 995
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 995: Reward 1.157 Loss 9.730 AE 0.026
Starting Iteration 996
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 996: Reward 1.159 Loss 10.855 AE 0.025
Starting Iteration 997
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 997: Reward 1.149 Loss 10.035 AE 0.025
Starting Iteration 998
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 998: Reward 1.146 Loss 9.254 AE 0.026
Starting Iteration 999
  Step 0/20
  Computing Advantages...
  Updating Policy...
Iter 999: Reward 1.151 Loss 9.432 AE 0.025
Saved checkpoint to policy_999.pth
Visualization updated. GIF saved at visualizations/training_evolution.gif
